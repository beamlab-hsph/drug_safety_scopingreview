Neural Computing and Applications (2019) 31:4799–4808
https://doi.org/10.1007/s00521-018-3722-8
(0123456789().,-volV)(0123456789().,-volV)

S.I. : EMERGENCE IN HUMAN-LIKE INTELLIGENCE TOWARDS CYBER-PHYSICAL
SYSTEMS

Detecting adverse drug reactions from social media based on multichannel convolutional neural networks
Chen Shen1 • Hongfei Lin1 • Kai Guo1 • Kan Xu1 • Zhihao Yang1 • Jian Wang1
Received: 26 April 2018 / Accepted: 10 September 2018 / Published online: 17 September 2018
Ó The Natural Computing Applications Forum 2018

Abstract
As one of the most important medical field subjects, adverse drug reaction seriously affects the patient’s life, health, and
safety. Although many methods have been proposed, there are still plenty of important adverse drug reactions unknown,
due to the complexity of the detection process. Social media, such as medical forums and social networking services,
collects a large amount of drug use information from patients, and so is important for adverse drug reaction mining.
However, most of the existing studies only involved a single source of data. This study automatically crawls the information published by users of the MedHelp Medical Forum. Then combining it with disease-related user posts which
obtained from Twitter. We combine different word embeddings and utilize a multi-channel convolutional neural network to
deal with the challenge that encountered in data representation of multiple sources, and further identify text containing
adverse drug reaction information. In particular, in this process, to enable the model to take advantage of the morphological
and shape information of words, we use a convolutional channel to learn the features from character-level embeddings of
words. The experiment results show that the proposed method improved the representation of words and thus effectively
detects adverse drug reactions from text.
Keywords Adverse drug reactions  Social media  Multi-channel convolutional neural network  Character-level
embeddings

1 Introduction
Adverse drug reactions (ADRs) are undesired harmful
results from a medication or other interference, such as
surgery. It is estimated that over 2 million serious ADRs
occur among hospitalized patients each year, resulting in
over 100,000 deaths in the USA [1]. ADRs are becoming
one of the leading causes of death in the USA and other
developed countries. Nevertheless, more than 50% of
severe adverse drug reactions remain undetected, causing
great harm to the people’s health and safety. Therefore, the
study of adverse drug reactions has always been the focus

& Hongfei Lin
hflin@dlut.edu.cn
1

School of Computer Science and Technology, Dalian
University of Technology, Dalian, China

of medical institutions, but it is complex, expensive and
time-consuming; a new method is required.
With the advent of the era of big data, more and more
medical research is considering using it for adverse reactions mining, such as through spontaneous reporting systems (SRSs), electronic medical records (EMRs), and
chemical/pharmacological databases [2]. SRSs are the
major source of ADR detection, as they offer a significant
amount of reliable and normalized data. However, the
inactiveness of these systems means there is an extremely
high under-reporting ratio. It is especially difficult to detect
new and emerging signals because a large number of
interesting cases cannot be timely collected due to the
under-reporting within the current reporting system.
Research based on electronic medical records has also been
developed, for example, Chan et al. [3] retrieved drug
utilization reports from EMRs to determine the patients
who were prescribed with antidepressants and oral anticancer drugs between 2006 and 2009 at one cancer center.

123

4800

Their findings showed that 10 out of the 17 antidepressantoral anticancer drug pairs could potentially cause pharmacokinetic interactions, and the rest were pharmacodynamics interactions, with only 3 out of the 17 drug pairs
being clinically documented to cause interacting events.
However, EMRs are often difficult to access because of
privacy issues. They are usually available only to those
research groups that have cooperation with hospitals,
clinics or other health organizations. The use of chemical/
pharmacological databases for mining ADRs is common
and can achieve remarkable results, but this method relies
on information such as the drug ingredients and molecular
structure, rather than text features.
Comparing with traditional data sources, the Internet not
only allows patients to share their experiences with each
other but also provide tremendous amount of ADR text
information which is available to the researchers. As a
result, the advantages of the Internet make it a valuable
source for the study of ADRs. However, dealing with
online texts also has its unique challenges. Similar to SRSs
and EMRs, the text containing adverse drug reactions are
without annotation. Although there are automated text
annotation methods that require less human resources, such
methods are generally limited in the scope of application
and can cause data imbalances. Raw text obtained from
Internet often contains a lot of noise, slang, unstructured
grammar, ad hoc abbreviations, phonetic substitutions, etc.
Word embeddings trained on public sources such as
PubMed have difficulty representing these vocabularies
and grammars.
This work proposes a method based on natural language
processing (NLP) and deep neural networks (DNNs) to
mine ADRs from social media. We crawled a number of
posts and comments about drugs from MedHelp (https://
www.medhelp.org), collected tweets about ADRs. We built
a medical dictionary by using domain resources, such as
side effect resource (SIDER), the consumer health vocabulary (CHV). With the help of the dictionary, we annotated
part of the dataset. Then, we use annotated tweets to balance our dataset. In order to solve the noise brought by
twitter text, we combined different word embedding
methods, using multi-channel convolutional neural network (CNN) to learn features from word embedding and
combine the manual features for ADRs identification. The
feature set includes term frequency–inverse document
frequency (TFIDF) and distributed word representation by
Skip-gram model, randomly initialized character embeddings for character-level sentence representation, statistical
characteristics and other textual sentiment features. Finally,
we built a multi-channel convolutional neural network to
classify the ADR-related post texts from MedHelp.
Experimental results show that our method is very effective
in detecting ADR information.

123

Neural Computing and Applications (2019) 31:4799–4808

2 Related work
Since social media constitutes a very large ADR data
resource, ADR mining from social media has received a
great deal of attention from researchers and many studies
have been done in this area.
Jiang et al. [4] used a machine learning-based technique
to classify personal experience tweets and used MetaMap
to recognize and extract word phrases connected to drug
effects. The results show that the technique can correctly
extract potential drug effects from Twitter data. Yang et al.
[5] proposed to use association mining and proportional
reporting ratios to mine the associations between drugs and
adverse reactions from the user-contributed content on
social media and FDA alerts were used as a gold standard
to test the performance of the proposed techniques. Leaman
et al. [6] mined the relationships between drugs and
adverse reactions as reported by the patients themselves in
user comments on health-related web sites and evaluated
the system with a manually annotated set of user comments, with promising results. Social media can also be
utilized for public health monitoring. Nikfarjam et al. [7]
used conditional random fields to extract mentions of
ADRs from social media. The literature shows that the
study of ADRs information in social media has significant
value. However, compared with traditional medical literature and clinical data, social media corpora have much nonADR-related content and contain a large number of nonstandard express, which makes ADR mining more
challenging.
In response to this, different vector representations of
words and character levels are used as inputs to feed the
multi-channel CNN model for the classification task. In
Kim’s [8] study, an additional channel is added to encode
general semantic similarities. By using the additional
channel for the non-static portion in the model, its performance of sentence classification has been improved on
multiple datasets. Zhang et al. [9] treated text as a kind of
raw signal at character level. Their proposed model outperforms traditional text classification methods using a
character-level CNN. Santos et al. [10] improved part-ofspeech (POS) tagging performance by associate characterlevel representation of words with common word representations. At tagging time, their model generates character-level embeddings for each word, even for the ones that
are outside the vocabulary. Ruder et al. [11] proposed a
model that combines character and word channels for
authorship attribution, which leveraged both stylistic and
topical information. The literature shows that CNN models
with character-level representation are effective for NLP
tasks and can outperform traditional methods under certain
conditions. The usage of multi-channel CNNs provides a

Neural Computing and Applications (2019) 31:4799–4808

way to learning features from word representation of both
word and character levels.

3 Method

4801

selected five most popular drugs from the forum. Then, we
downloaded users’ posts and comments about these drugs
with the crawler. Following the collection of the data, a
randomly selected sample of the data was chosen for
annotation, which consisted of 3122 instances. Details are
described in Sect. 3.4.

3.1 Data acquisition
3.1.2 Twitter-related corpus
3.1.1 MedHelp medical forum-related data
Social networks have been technologically advanced for
many years in the field of disease and health. They have
accumulated a lot of users and community groups and
gathered users sharing their health and medication information with others. Established in 1994, MedHelp has
become the pioneer in online health community, which was
designed to help patients and consumers to communicate
health information on the Internet. The forum had attracted
more than 12 million users and more than 10 million posts
by 2012. It is operated in the form of question and answer.
The contents are divided according to the kinds of drugs
and diseases. Figure 1 presents an example of drug information and related posts of methadone. In this study, we

As one of the largest information exchange platform,
Twitter has attracted a large number of users to share
personal information. By 2018, Twitter’s monthly active
users had reached 330 million. They access Twitter regularly. Forty-two percent of Twitter users access Twitter
every day. Due to the huge numbers of users and frequent
use, the massive tweets are inevitable to contain lots of
valuable information on adverse drug reactions, which has
been widely concerned by researchers. In the previous
study, Biomedical Informatics Laboratory at Arizona State
University [12] collected a part of the disease-related
tweets, and the experts manually annotated them, so that
the results are of high authority and accuracy. Therefore,
this paper uses these data as the original data.

Fig. 1 Drug information and
discussions of methadone on
MedHelp

123

4802

3.2 Preprocessing
The original text obtained from Twitter and Web Forums
contains lots of meaningless words or characters, such as
URLs, some non-alphabetic characters like ‘‘*?&/’’, which
decrease the accuracy of detection. After screening the
noise, there are still some words which are not related to
drugs or ADRs in the data, such as prepositions, and verbs.
These grammars form human-readable texts, but previous
studies have shown that bypassing these words can reduce
the model’s solution space and improve performance.
Consequently, we compiled the dictionary of stop words to
filter these words. An example of our result after preprocessing is provided in Table 1.

3.3 Compilation of medical dictionary
How to recognize medical text entity from the original text
has been one of the key problems to be solved in the
medical field. With the rapid developed of big data, some
organizations have built a variety of high-quality databases
in the medical field. Thus, medical entity identification
based on dictionary has achieved high accuracy and got a
great advantage in terms of speed. In this work, we generated a medical dictionary by combining several authoritative and comprehensive databases.
SIDER [13] is a database of drugs and drug adverse
reactions, which contains 1430 kinds of drugs and 5880
kinds of ADRs. We compiled a drug dictionary and an
ADR dictionary separately. Besides, we added the entities
from the SIDER.
The Consumer Health Vocabulary (CHV) [14] mainly
provides the correspondence between non-professional
vocabulary and medical entities. It enables applications to
translate technical terms into user-friendly languages that
are often used on social media. CHV contains more than
150,000 matching entities and covers massive medical
texts, which can match different expressions to same
entities. It can decrease the dictionary dimensions of corpus
and make the result more accurate. Table 2 shows the
examples of CHV entities.
MedHelp is a professional medical forum, which separates the threads according to the kind of drugs. In this

Neural Computing and Applications (2019) 31:4799–4808

case, users who use the same drugs can discuss on the same
thread. To cover a wider range of user group, the forum not
only provides a list of 22,919 drugs, covering a majority of
commercial drugs but also collects a valuable corpus for
ADRs researches. As a result, we obtained the list of drugs
and combined it with the SIDER drug dictionary.

3.4 Feature extracting
3.4.1 Text segmentation and annotation
As the context in social media, the tweets and user posts
from Twitter and MedHelp Forum share similar structure
and characteristics, but they still have some differences.
Twitter allows a maximum of 140 words for one post. In
comparison, as a typical forum, the posts and comments in
MedHelp have no word limit. However, it is found that the
occurrence of ADRs mostly correlates with the sentences,
and the analysis of the whole document may lose the
information of ADRs. In consequence, this study took the
sentences as the samples for mining adverse drug reactions.
Considering that some sentences have a semantic relevance
to their context, we utilized the sliding window of size-2 to
link adjacent two sentences as a sample, after separating
the sentences using Natural Language Tool Kit (NLTK)
sentence tokenizer. After the processing, MedHelp text
corpus achieved a similarity to that of Twitter in word limit
and semantic integrity. Therefore, we manually annotated
more than 3000 MedHelp sentences which are not ADRs
related as a negative sample and combined the positive
samples from tweets (1013 tweets) to build the training
dataset. The final dataset is imbalanced, with 1013 (24.5%)
samples containing ADR mentions and 3122 (75.5%)
containing no ADR mentions.
The part of annotated dataset is shown in Table 3.
3.4.2 Recognition of drug and adverse drug reactions
This study was aimed to compile a comprehensive dictionary of drug and adverse reactions, with which the vast
majority of drugs and their adverse reactions can be identified. Considering that the word length of a drug or an
adverse reaction is mostly 1 or 2, we split the cleaned text

Table 1 Result of data preprocessing
Original text

Cleaning result

@upasbook Great read as always. I was on Cymbalta for 5 days. Cold turkey had
sweats, migraine, tremors while on & 3 days after

@upasbook great read always. Cymbalta cold turkey
sweats, migraine tremors while after

Rivaroxaban diary day 22. Last tablet taken Tuesday 18th, 4 days ago. Woken up
in pain. Bad fluid retention (uneven) on both legs

rivaroxaban diary. Last tablet taken Tuesdays, woken up
pain. Bad fluid retention uneven legs

123

Neural Computing and Applications (2019) 31:4799–4808
Table 2 Examples of CHV

4803

ADRs text

Matching result

Cardiac arrest, arrest heart, asystole, ventricular asystole

Heart arrest

Muscle weaknesses, weakness muscles, muscular weakness

Muscle weakness

Chorea minor, rheumatic chorea, chorea Sydenhams

Sydenham’s chorea

Table 3 Examples of annotated samples
Text

ADRs
related

‘‘@johncheese I hated effexor. It makes you hungry, dizzy, and lethargic. That culminated in a large weight gain for me.’’’

Yes

@DrOz i take Trazodone for my #insomnia & still have a hard time falling asleep or I end up having crazy dreams that wake me
up!?! #OzSleep

Yes

He was at home for the first year and he did not fall sick even once. Now it seems like he has some sort of cold or flu pretty much
once a month

No

into a length of 2-word sequence, matched it to the drug
and adverse reaction entity. Hence, we used both the
number of drug and the number of adverse reactions as
features of the sample.
3.4.3 Recognition of sentiment words
Unlike the traditional medical literature and clinical data,
the textual resources of social media are created by a large
number of non-professional users, without strict professionalism, which contain a lot of sentiment information.
This information not only enriches the emotional expression of users, but also adds a wealth of features in the
description of adverse reactions. In view of this, this study
also used sentiment words as an important feature of text
analysis, so as to enhance the accuracy of the model.
Most of the sentiments related to the medical text are
negative, so the judgment of sentiment polarity has little
contribution to the task. In addition, the intensity of sentiment can be used as a feature to measure adverse reactions, which has a positive effect on the discovery of
adverse reactions. In this study, we applied the English
sentiment dictionary from NLTK to extract the sentiment
words from the text, assigned the sentiment scores and then
weighted them as the sentiment features.

3.5 Text representation
For machine learning model, the text should be represented
as the form of fixed-length vector which can be calculated
as the feature input. In the field of text processing, common
text representation methods mainly include TFIDF and
Word2Vec.

Due to their domain characteristics, drug or adverse
reactions entities show a low appearance frequency in the
medical text, which also brings a series of problems to text
processing. For instance, the distributed vector is too
sparse, which affects the training precision of the model.
On account of this, before the textual representation of the
text, we mapped the drugs and adverse reactions entities
into uniform words, such as ‘‘Drug’’ and ‘‘Adverse’’, using
the dictionaries we compiled.
3.5.1 TFIDF vector
Term frequency–inverse document frequency (TFIDF) is a
numerical measurement intended to reflect the importance
of a word to a document in a collection or corpus. It is often
used as a weighting factor in information retrieval, text
mining, and user modeling. The TFIDF value increases
proportionally with the occurrence number of a word in the
document, but it is often offset by the frequency of the
word in the corpus. This helps to adjust the phenomenon
that some words appear more frequently in general.
Nowadays, TFIDF is one of the most popular termweighting patterns, and the corpus has been transformed to
TFIDF vector of about 40,000 dimensions. Since the
TFIDF vector of each text is too large and sparse, it affects
the computational efficiency and the accuracy of the
model. For this reason, we utilized the singular-value
decomposition (SVD) to reduce the dimensions of the
vector to 256.
3.5.2 Word-level embeddings
Typically, classification and clustering algorithms require
text input to be represented as a fixed-length vector. The

123

4804

common models that meet this requirement are bag-ofwords and bag-of-n-grams. Bag-of-words would undoubtedly lose any meaning that might come from word order.
For example, ‘‘Contain the order highly intuitively sense
you likely information to is useful that word.’’ (Intuitively,
people can sense that word order is highly likely to contain
useful information). Bag-of-n-grams would only consider
short contexts and suffer from data sparsity and high
dimensionality. In view of this, Google has proposed a
Skip-gram model [15, 16] based on deep learning. This
method can represent a word as a fixed-length vector. The
distance between two vectors can be considered as the
sentiment difference between the words. This method has
been used widely in the field of sentiment analysis and text
classification. Nevertheless, Skip-gram model is a wordlevel representation which can only transform a word into a
vector. For the analysis on a document or a sentence-level
text, further processing will be required.
Researchers have tried to combine distributed word
vectors previously in many years, such as by using a
weighted average of all words in a document, and by
integrating word vectors in an order given by the parse tree
of a sentence. The first of these approaches also suffered
from a loss of word order information, and the latter one
could not easily be extended beyond sentences.
In this study, we use the concatenated Skip-gram vectors
that pre-trained on PubMed corpus to represent sentences
as word-level representation and use a multi-channel CNN
to extract the features, that the word order and sentiment
information can be preserved.
3.5.3 Character-level embeddings
To get a better representation of words from social media,
beyond the contextual information of words, the morphological and shape information must take into consideration.
This information exists in the combination of all characters
of words, which can be extracted by character-level representation. For instance, in the generic naming systems of
drug, these features may appear in the beginning (e.g., the
‘‘cef’’ in ‘‘cefazolin’’ is the affix for cephem-type antibiotics), in the middle (e.g., the ‘‘grel’’ in ‘‘ticagrelor’’ is for
platelet aggregation inhibitor), or at the end (e.g., the
‘‘olol’’ in ‘‘atenolol’’ is for beta-blockers) of drug names.
We use a trainable random initialized vector to represent
characters in our study, thus the character-level representation can be updated by the backpropagation algorithm
during training.

3.6 Model building
In recent years, deep learning has exerted a marvelous
influence in the field of text, and lots of natural language

123

Neural Computing and Applications (2019) 31:4799–4808

processing tasks have been completed with the help of deep
learning. By simulating the neuron interconnection structure in human brain, small computational units can form a
complex and ordered neural network model, which can
learn a variety of potential expressions from data.
3.6.1 Framework of convolutional neural network
CNN is a special form of neural network developed in
recent years. In the 1960s, when Hubel and Wiesel were
studying the local sensitive neurons and directional selection in the cortex of cats, they found that this type of
network structure could effectively reduce the complexity
of the feedback neural network. On this basis, they proposed the schema of convolutional neural network. In
general, the basic structure of CNN consists of two kinds of
layers. One is the feature extraction layer. The input of
each neuron is connected to the local acceptance domain of
previous layer, which extracts the local features. Once the
local features are extracted, their positional relationship
with other features is also determined. The other is the
feature mapping layer. Each calculation layer of the network is composed of multiple feature maps. Each feature
map is a plane, and the weights of all neurons on the plane
are equal. The feature mapping structure uses sigmoid
function, which affects the function nucleus as activation
function of the convolution network. Consequently, the
feature map has the invariance of displacement. In addition, since the neurons share weights on a map surface, the
parameters numbers of the network is reduced. Each convolution layer in a convolutional neural network follows a
computational layer for local averaging and secondary
extraction. This unique feature extraction structure reduces
the feature resolution.
3.6.2 Multi-channel CNN
The proposed work built the sentence-level text classifier
based on multi-channel CNN. We input word-level and
character-level word embeddings into different CNN
channels, extract features by using multiple filters and then
extract the most important features in the feature map by kmax-pooling. In this study, we set the number of convolutional filters in the two channels to 64, so that after the
pooling layer, we get the sentence representation of the
same length as the TFIDF vector. Then we connect the
fixed-length sentence representation obtained by k-maxpooling with TFIDF vector, sentiment score, and entity
count feature. The merged features work as input into the
final full-connected prediction (sigmoid) layer of the CNN
to classify the text. The framework of CNN is shown in
Fig. 2.

Neural Computing and Applications (2019) 31:4799–4808

4805

Fig. 2 Framework of the
proposed model

Provided that a sentence X ¼ x1 ; x2 ; x3 ; . . .; xm , xi 2 RD
is a D-dimensional word vector corresponding to the i-th
word in the sentence, the kernel with a parameter matrix
W1 2 RHCD and bias vector b1 2 RH convolves the C
continuous words xi:iþC1 from x1 to xN in the convolution
layer. H is the output unit number of the kernel; the kernel
output mk can be computed using the following equation:
mk ¼ tanhðW1 ei:iþC1 þ b1 Þ

ð1Þ

wherein D, C and H are the hyper-parameters of the model.
3.6.3 Pooling layer
A k-max-pooling layer [17] subsequently reduce the
dimension while extracting the features from CNN. The kmax-pooling layers can obtain the top-k strongest features
from each convolutional feature map. The conventional
max-pooling layer can be represented as follow:
fj ¼ max mkj
1kK

ð2Þ

wherein j indicates the j-th output of the pooling layer,
namely the j-th component of the output vector of CNN.
This means that the conventional pooling method can only
acquire one of the features for each feature map. Conversely, given a sequence p 2 Rm of length m and a value k,
k-max-pooling selects the k highest values of p. In the
result sequence pkmax , the values remain the original order in
p. In this way, the k-max-pooling method retains certain
position information while obtaining k important features.
In this study, we set k = 2 by preliminary parameter tuning.
Finally, the sentence is represented as a fixed-length
vector through convolution and pooling layers.

3.6.4 Activation and loss function
The hidden layer utilizes the Rectified Linear Unit (ReLU)
activation function [18]. During backpropagation, many
older activation functions, such as the logistic sigmoid or
hyperbolic tangent, may produce a gradient of zero for
large inputs, which can slow or even halt the learning
process. The ReLU function doesn’t saturate for large
positive inputs. Thus, the ReLU function is often used to
avoid the problem of vanishing gradient. It gives an output
z if z is positive and 0 otherwise, which is defined as
follows:
reluðzÞ ¼ maxð0; zÞ

ð3Þ

Given the input vector x, the hidden layer makes a nonlinear combination, which can be formulated as follows:
h ¼ reluðW2  x þ b2 Þ

ð4Þ

wherein W2 2 RH2 G is a parameter matrix and b2 2 RH2 is
a bias vector. H2 is a hyper-parameter, denoting the number
of nodes in the hidden layer.
The output layer calculates the probability that a sentence is ADRs related. Therefore, we used the sigmoid to
compute the maximum probability:
Pðsi Þ ¼ sigmoidðsi Þ ¼

1
1 þ eW3 h

ð5Þ

wherein si denotes the i-th sentence of the samples, and W3
signifies the parameter matrix.
The model trained with the binary cross-entropy loss
function is shown in Eq. 4. We used the Adam [19] optimizer with a learning rate of 0.0001 to optimize the
training objective. A dropout [20] of 20% is utilized to
prevent overfitting. The batch size is set to 32.

123

4806

Neural Computing and Applications (2019) 31:4799–4808

crosssentropyðt; oÞ ¼ ðt  logðoÞ þ ð1  tÞ  logð1  oÞÞ
ð6Þ

4 Result and error analysis
We evaluated the proposed model on the corpus annotated
with ADRs. Fivefold cross-validation is performed on the
dataset. For evaluation metrics, we compared the proposed
method and models with different experimental settings in
terms of precision, recall and F1-score on the dataset. We
also performed an error analysis of the sets of false positives (FPs) and false negatives (FNs) samples that will be
described at the end of this section.
To evaluate manual features, the results of the wordlevel channel with these features are shown in Table 4,
which gives the precision, recall and F1-score. The highest
value is highlighted in bold. WP presents the result of the
model which only use word-level channel of CNN for
prediction. WP ? TFIDF presents the result concatenated
the word-level channel with the TFIDF features. Then, we
added the sentiment feature (SF) and entity count feature
(EC) to evaluate the contribution of these features separately. The entity count feature includes both the number of
disease entities and the number of drug entities.
The experimental results indicate that by combining
TFIDF and word embeddings as inputs, the proposed
model can function as a feasible solution of identifying
ADRs. Adding the sentiment feature improves the performance of the model, since the posts often contain negative
emotions when it motions ADRs. We also discovered that
the entity count is an effective feature of the task. By
combining the TFIDF, sentiment feature and entity count
feature with word-level representations, we achieved a F1
score of 72.1% on a single-channel CNN.
The results of the proposed model are shown in Table 5.
To evaluate the performance of other components, a leaveone-out is performed.
The proposed model is denoted by ‘‘Multi-channel’’ in
Table 5. We removed the improvements that may affect
the results one by one to evaluate the impact of it on the

Table 5 Results of the proposed model
Method

Precision

Recall

F1 scores

w/o dropout

83.8

66.3

74.0

w/o k-max-pooling

83.5

65.9

73.7

w/o character-level channel

82.5

64.1

72.1

Multi-channel

84.0

66.7

74.4

proposed model. For instance, ‘‘w/o k-max pooling’’ means
removed the k-max-pooling layer (replaced with traditional
max-pooling) from the proposed model, and ‘‘w/o character-level channel’’ represents a model with word-level
channel only. The experimental results in Table 5 show
that the addition of character-level channel improves the
performance of the model and provide the most performance boost to the model compared with other techniques
we used. This suggests that in addition to the contextual
information of words, the model obtained useful features
by obtaining the morphological and shape information
generated by the character-level CNN channel. The effect
of replacing k-max-pooling with traditional max-pooling
on model performance is 0.7% that indicate k-max-pooling
also improved the learning ability of the model by providing more features.
We also evaluated the impact of the number of filters in
CNN. More filters usually mean more learnable features,
but the increase in model parameters and solution space
can cause the model difficult to train. The effect on the
results of the different number of filters is shown in Fig. 3.
In this study, we set the number of filters to 64. Figure 3
shows that when the number of filters is set to 64, the
model achieves the best performance in cross-validation,
and further increasing its number does not improve the
performance.

Table 4 Results of the experiment using only word-level CNN
channel
Method

Precision

Recall

F1-scores

WP

78.8

59.2

67.6

WP ? TFIDF

79.3

60.8

68.8

WP ? TFIDF ? SF

81.7

63.2

71.2

WP ? TFIDF ? SF ? EC

82.5

64.1

72.1
Fig. 3 The effect of the number of filters

123

Neural Computing and Applications (2019) 31:4799–4808

We also performed an analysis to determine the primary
sources of error under this method. A total of 100 sample
sentences were randomly selected and analyzed. Regarding
the detection of adverse events, the major cause of false
was the use of colloquial expressions or slangs to describe
an adverse event. For instance, tweets, such as ‘‘@AaronM
effexor was a boner killer, absolutely. welbutrin goes the
other way’’, are used by a user to describe the side effects
of the drugs. The word ‘‘killer’’ contains sentiment information, which word-level embeddings trained on PubMed
is difficult to learn. Character-level embeddings are also
difficult to deal with this circumstance and yield a wrong
prediction. However, when proceeding to the second sentence, the model with character-level channel returned a
higher confidence score with right prediction. This is
because the word ‘‘welbutrin’’ is a misspell for ‘‘wellbutrin’’ which is an unregistered word of word-level
embedding. On the contrary, trainable character-level
embeddings can learn the morphological features of words
through iterative training. Also, these phrases or words are
not included in our dictionary. Thus, the limitation of the
dictionary affected the accuracy of results. The second
highest cause of false is the emoji in the context. Emoji has
been widely used to express user’s emotions in the social
media. For example, ‘‘:)’’ and ‘‘:(’’ refer to happy or sad
mood, respectively. They appear frequently. Nonetheless,
we neglected this information in the step of data cleaning,
in which most non-alphabetic characters were screened.
Although preserving these characters allows the model to
learn some of the features through character-level representation, the introduced noise also reduces the accuracy of
word-level representations. A better way to use emoji
information remains to be studied.
The other factor of the incorrect classification is the
quality of the Skip-gram embedding vectors. In the task of
natural language processing, the training result of the
embedding vectors will have a relatively high effect on the
experiment, and the training of Skip-gram model requires
corpus to have a large scale. Also, the corpus used to train
word embedding should be similar to the experimental
dataset. In this work, we tried to recognize most drugs and
adverse reactions through lexicon. The processed corpus
still contains many medical entities, which appear at a low
frequency. Due to the mechanism of the model, the
embedding vectors of these words tend to show randomization. The results from this study suggest that word-level
word representations are difficult to deal with unregistered
words. In the absence of crawling a large amount of corpus
from social media, the use of character-level word representations can be used as a complement.

4807

5 Conclusion
In this research, we proposed a method to classify the
ADRs-related texts from social media. We utilized
biomedical databases to extract and clean the data from
tweets and MedHelp posts and then constructed the dictionary of drug and adverse reactions. Subsequently, we
transformed the cleaned corpus to the Skip-gram embeddings, character-level embeddings. We use multi-channel
CNN to learn the features in the representations, and then
combine the sentence-level features with other manual
features. The experimental results show that by providing
the morphological and shape features of words, the use of
multi-channel CNN and character-level word representations boosted the proposed model on classify the ADRsrelated medical texts.
The proposed model can be applied to the task of mining
side effects of drugs from social media. In the near future,
we plan to continue to experiment with a combination of
multiple word representation methods while gathering
training data that more suitable for social media data
mining tasks to further improve the performance of the
model.
Acknowledgements The authors acknowledge the National Natural
Science Foundation of China (Grant: 61572102), the National Natural
Science Foundation of China (Grant: 61632011), the National Natural
Science Foundation of China (Grant: 61572098).

Compliance with ethical standards
Conflict of interest The authors declare that they have no conflict of
interest.

References
1. Giacomini KM, Krauss RM, Roden DM, Eichelbaum M, Hayden
MR, Nakamura Y (2007) When good drugs go bad. Nature
446(7139):975–977
2. Yang H, Yang CC (2013) Harnessing social media for drug–drug
interactions detection. In: 2013 IEEE international conference on
healthcare informatics (ICHI). IEEE, pp 22–29
3. Chan A, Yap KYL, Koh D, Low XH, Cheung YT (2011) Electronic database to detect drug–drug interactions between antidepressants and oral anticancer drugs from a cancer center in
Singapore: implications to clinicians. Pharmacoepidemiol Drug
Saf 20(9):939–947
4. Jiang K, Zheng Y (2013) Mining Twitter data for potential drug
effects. In: International conference on advanced data mining and
applications. Springer, Berlin, pp 434–443
5. Yang CC, Yang H, Jiang L, Zhang M (2012) Social media mining
for drug safety signal detection. In: Proceedings of the 2012
international workshop on Smart health and wellbeing. ACM,
New York, pp 33–40
6. Leaman R, Wojtulewicz L, Sullivan R, Skariah A, Yang J,
Gonzalez G (2010) Towards internet-age pharmacovigilance:
extracting adverse drug reactions from user posts to health-related

123

4808

7.

8.
9.

10.

11.

12.

13.

social networks. In: Proceedings of the 2010 workshop on
biomedical natural language processing. Association for Computational Linguistics, pp 117–125
Nikfarjam A, Sarker A, O’connor K, Ginn R, Gonzalez G (2015)
Pharmacovigilance from social media: mining adverse drug
reaction mentions using sequence labeling with word embedding
cluster features. J Am Med Inform Assoc 22(3):671–681
Kim Y (2014) Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882
Zhang X, Zhao J, LeCun Y (2015) Character-level convolutional
networks for text classification. In: Advances in neural information processing systems, pp 649–657
Santos CD, Zadrozny B (2014) Learning character-level representations for part-of-speech tagging. In: Proceedings of the 31st
international conference on machine learning (ICML-14),
pp 1818–1826
Ruder S, Ghaffari P, Breslin JG (2016) Character-level and multichannel convolutional neural networks for large-scale authorship
attribution. arXiv preprint arXiv:1609.06686
Sarker A, Gonzalez G (2015) Portable automatic text classification for adverse drug reaction detection via multi-corpus training.
J Biomed Inform 53:196–207
Kuhn M, Campillos M, Letunic I, Jensen LJ, Bork P (2010) A
side effect resource to capture phenotypic effects of drugs. Mol
Syst Biol 6(1):343

123

Neural Computing and Applications (2019) 31:4799–4808
14. Zeng QT, Tse T (2006) Exploring and developing consumer
health vocabularies. J Am Med Inform Assoc 13(1):24–29
15. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013)
Distributed representations of words and phrases and their compositionality. In: Advances in neural information processing
systems, pp 3111–3119
16. Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781
17. Hui K, Yates A, Berberich K, de Melo G (2018) Co-pacrr: a
context-aware neural ir model for ad-hoc retrieval. In: Proceedings of the eleventh ACM international conference on web search
and data mining. ACM, New York, pp 279–287
18. Nair V, Hinton GE (2010) Rectified linear units improve
restricted boltzmann machines. In: Proceedings of the 27th
international conference on machine learning (ICML-10),
pp 807–814
19. Kingma D, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980
20. Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R (2014) Dropout: a simple way to prevent neural networks
from overfitting. J Mach Learn Res 15(1):1929–1958

