Computational Statistics & Data Analysis 34 (2000) 473‚Äì493
www.elsevier.com/locate/csda

Bayesian neural networks with
con dence estimations applied
to data mining
R. Orre a;‚àó , A. Lansner a , A. Bateb , M. Lindquistb
a

SANS, Dept. of Computer Science, Royal Institute of Technology, S-100 44 Stockholm, Sweden
b
WHO Collaborating Centre for International Drug Monitoring, Uppsala Monitoring Centre,
S-753 20 Uppsala, Sweden
Received 1 September 1998; received in revised form 1 August 1999; accepted 26 November 1999

Abstract
An international database of case reports, each one describing a possible case of adverse drug reactions (ADRs), is maintained by the Uppsala Monitoring Centre (UMC), for the WHO international
program on drug safety monitoring. Each report can be seen as a row in a data matrix and consists
of a number of variables, like drugs used, ADRs, and other patient data. The problem is to examine
the database and nd signi cant dependencies which might be signals of potentially important ADRs,
to be investigated by clinical experts. We propose a method by which estimated frequencies of combinations of variables are compared with the frequencies that would be predicted assuming there were
no dependencies. The estimates of signi cance are obtained with a Bayesian approach via the variance
of posterior probability distributions. The posterior is obtained by fusing a prior distribution (Dirichlet
of dimension 2n‚àí1 ) with a batch of data, which is also the prior used when the next batch of data
arrives. To decide whether the joint probabilities of events are di erent from what would follow from
the independence assumption, the ‚Äúinformation component‚Äù log(Pij =(Pi Pj )) plays a crucial role, and
one main technical contribution reported here is an ecient method to estimate this measure, as well as
the variance of its posterior distribution, for large data matrices. The method we present is fundamentally an arti cial neural network denoted Bayesian con dence propagation neural network (BCPNN).
We also demonstrate an ecient way of nding complex dependencies. The method is now (autumn
1998) being routinely used to produce warning signals on new unexpected ADR associations. c 2000
Published by Elsevier Science B.V. All rights reserved.

‚àó

Corresponding author.

0167-9473/00/$ - see front matter c 2000 Published by Elsevier Science B.V. All rights reserved.
PII: S 0 1 6 7 - 9 4 7 3 ( 9 9 ) 0 0 1 1 4 - 0

474

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

1. Introduction
The fundamental aim is to nd new unexpected dependencies between variables
in a database. The database, which this methodology has been implemented on, consists of case reports of adverse drug reactions, reported from 50 WHO collaborating
national centres. This database currently contains nearly two million reports, in each
report more than 77 variable elds may be considered for analysis.
The database is updated quarterly with approximately 35 000 reports. Primarily
we want to nd new unexpected associations in the data set occurring due to this
quarterly update of the database. Initially between drugs or combinations of drugs
and adverse reactions or combinations of adverse reactions, but also including other
variables like country and patient age.
For this purpose we have extended a Bayesian neural network (Lansner and Ekeberg, 1989; Holst and Lansner, 1995) to be able to do estimations of variances
of weights and posterior distributions to be suitable for data mining. The Bayesian
neural network we use here is a feed forward network 1 where the learning and inference rules are based upon Bayes rule (Bayes, 1763; Laplace, 1825) for conditional
probabilities. We want to nd the posterior probability function for an outcome or
response variable A which is conditioned by a joint input event or explanatory variable D under the assumption that we can express the joint likelihood density P(D|A)
as a product of n independent marginal densities P(di |A) as
P(A|D) = P(A)

P(D|A)
P(d1 |A) ¬∑ P(di |A) ¬∑ ¬∑ ¬∑ P(dn |A)
= P(A)
:
P(D)
P(D)

(1)

The outcome A may in general be represented by a continuous distribution, but here
we deal with discrete outcomes. In the following, A means the set of mutually exclusive outcomes a1 ; aj ; : : : ; am . We use the symbol ‚ÄúA‚Äù as this often represents adverse
drug reactions or a combinations thereof in our application. The input events di most
often represents drugs or combinations of drugs. Such a network is fundamentally a
naive Bayesian classi er (Good, 1950) but it has earlier been extended with higher
order units that deals with classi cation and diagnosis also for tasks involving dependent inputs (Lansner and Holst, 1996), where it was denoted Bayesian con dence
propagation neural network (BCPNN). Here we extend the latter by calculating also
the variance
V

P(aj |di )
P(aj )

!

=V

P(di ; aj )
P(di )P(aj )

!

(2)

which is a particularly useful measure, for instance, when we do data mining, particularly on associations with low-frequency counts, where the uncertainty may be
large. Similarly we calculate V (P(aj |D)), the variance for a posterior probability,
which gives us a con dence measure of a prediction or classi cation task. Following
established practice in the area of Bayesian neural networks, we use E(P(A|D)),
1

It has also been used as a recurrent Hop eld-like network useful for pattern completion (Lansner and
Ekeberg, 1985,1989; Kononenko, 1989).

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

475

V (P(A|D)), etc., to denote the mean and variance of the posterior distributions of
P(A|D).
The Bayesian feed forward neural networks have similarities to Bayesian belief
networks (Pearl, 1988) and they can theoretically be transformed into each other
(Holst and Lansner, 1995). The main di erence is that in the latter only the dependent variables are dealt with in each node, whereas in the neural network model
both dependent and independent variables are treated in parallel and the result is
propagated through a few layers only.
1.1. Bayesian inference in BCPNN
Let the response variable A be composed of m mutual exclusive outcome events
aj . Bayes rule gives the following relation
P(aj |D) =

P(aj )P(D|aj )
P(aj )P(D|aj )
:
=P
P(D)
j P(aj )P(D|aj )

(3)

Then, let the joint explanatory event D be composed of n independent events di ,
such that
P(D|aj ) = P(d1 |aj ) ¬∑ P(d2 |aj ) ¬∑ ¬∑ ¬∑ P(dn |aj ):
Each event di is further made up of Ki mutually exclusive sub-events, or states, such
that
P(di |aj ) = P(d1i |aj ) + P(d2i |aj ) ¬∑ ¬∑ ¬∑ P(dKi i |aj ):
These assumptions, i.e. P(dki |aj ) being independent over i and mutual exclusive over
k gives
P(D|aj ) =

YX
i

k

P(dki |aj );

(4)

which, using Bayes rule, can be rewritten as
P(D|aj ) =

Y X P(aj |dk )
i
i

P(aj )

k

P(dki ):

(5)

Now, replace P(aj |dki ) above with P(dki ; aj )=P(dki ) = P(aj ; dki )=P(dki ) (de nition) and
on event dki during
P(dki ) with its belief value ik , which expresses the current belief
Pk=Ki k
training and inference in an exhaustive way, i.e. [dki ‚â• 0; k=1 i = 1]. A binary
variable is thus represented by one ‚Äúon-unit‚Äù and one ‚Äúo -unit‚Äù. We get
P(D|aj ) =

Y X P(dk ; aj )
i
i

k

P(dki )P(aj )

ik ;

(6)

which we generally consider approximately valid also when P(dki |aj ) are almost
independent over i. Using Eq. (6), Eq. (3) can now be written
P(aj |D) Àô P(aj )

YX
i

k

P(dki ; aj ) k
;
P(dki )P(aj ) i

(7)

476

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

which resembles many feed forward arti cial neural network architectures. For discrete belief values (dki ‚àà {0; 1}) we may use the following simpli ed form:
"

P(aj |D) Àô exp log P(aj ) +

XX
i

k

"

#

#

P(dki ; aj )
log
k :
P(dki )P(aj ) di

(8)

In the last expression (8) we would recognize ‚Äúexp‚Äù as the transfer function and
‚Äúlog P(aj )‚Äù as a bias term from other arti cial neural network architectures. The
corresponding weight value is then either [log P(dki ; aj )=P(dki )P(aj )] as in (8) or just
[P(dki ; A)=P(dki )P(aj )] as in (7). Which equation to prefer depends on the application.
For precise mixture modelling of, e.g. continuous variables (Orre and Lansner, 1996)
preferably (7) be used, due to the better accuracy, rather than (8). The logarithmic
form in (8) has been used a lot in, e.g. recurrent networks (Lansner and Ekeberg,
1989) and also in classi cation (Holst and Lansner, 1995). In the data mining application described here and in Bate etal. (1998) we use the logarithmic form, as this
has a nice connection with information theory, especially mutual information (Pearl,
1988). Therefore, we refer to the term [logP(dki ; A)=P(dki )P(A)] as information component because it is a measure of the information that migrates from one state of a
variable to one state of another variable. Mutual information in its discrete form can
then be regarded as a weighted sum of information components,
XX
P(x; y)
I (X ; Y ) =
:
(9)
P(x; y)log
P(x)P(y)
x
y
In the rest of this paper we use the following de nitions of the weights and information components (observe that index k is assumed but not always included
throughout this text):
Wij =

P(dki ; aj )
;
P(dki )P(aj )

ICij = log Wij :

(10)
(11)

2. Method to estimate probabilities and uncertainties
We start by estimating the probability for a single binary event of a Bernoulli trial
represented by a variable with outcomes 0 and 1. The likelihood function for c1 , i.e.
the probability to get c1 number of outcome 1 from a total of C = c0 + c1 trials, is
a binomial distribution:
 
C
P(c1 |p1 ; C) =
p1c1 (1 ‚àí p1 )c0 :
(12)
c1
In the classical perspective we get the maximum of the likelihood by di erentiating
vs p1 and solve (d=dp1 )P(c1 |p1 ) = 0 as
c1 (1 ‚àí p1 ) = c0 p1 ;
pÃÇ1 =

c1
c1
= :
c0 + c1
C

(13)

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

477

This classical estimate does, however, not give us accurate estimates of p1 for small
counter values and does not tell us anything about the signi cance of an estimated
probability. To overcome this we use the Bayesian method to assert an a priori
probability distribution for the variable, which is re ned when more information,
i.e. samples, become available. We consider p1 to be drawn from a conjugate family
of distributions, which we assert as the prior distribution. A convenient prior which
is much used if we do not expect the input to be a multi modal mixture (Bernado
and Smith, 1994; Heckerman, 1997) is the Beta distribution, described by hyperparameters 1 and 0
P(p1 ) =

( 1 + 0 ) 1 ‚àí1
p
(1 ‚àí p1 )
( 1) ( 0) 1

0 ‚àí1

(14)

which gives a posterior for p1 , given the counters c1 and c0 , which is also a Beta
(Bernado and Smith, 1994):
P(p1 |c1 ; c0 ) =

(C + 1 + 0 )
pc1 +
(c1 + 1 ) (c0 + 0 ) 1

1 ‚àí1

(1 ‚àí p1 )c0 +

0 ‚àí1

:

(15)

The expectation value pÃÇ1 = E(p1 ) we get by integration and normalization, where
the reduction makes the s disappear:
R1

E(p1 ) =

0

p1 ¬∑ p1c1 +

1 ‚àí1

R1

c1 + 1 ‚àí1
(1
0 p1

(1 ‚àí p1 )c0 +
‚àí p 1 )c 0 +

0 ‚àí1

0 ‚àí1

dp

dp

:

(16)

The solution to this [where Beta(x; y) = (x) (y)= (x + y)] is
E(p1 ) =

Beta(c1 + 1 + 1 ; C ‚àí c1 + 0 )
Beta(c1 + 1 ; C ‚àí c1 + 0 )

which simpli ed gives the following ( =
pÃÇ1 = E(p1 ) =

c1 +
C+

1

:

1

(17)
+

0)

for pÃÇ1 :
(18)

In the same way we nd the variance estimation (ÀÜ2p = V (p) = E(p2 ) ‚àí E(p)2 ) and
the estimate of V (p1 ) becomes
V (p1 ) =

(c1 + 1 )(C ‚àí c1 + ‚àí 1 )
:
(C + )2 (1 + C + )

(19)

2.1. Joint probabilities
As a prior for the joint probability pij , which has four di erent outcomes, we assert
a three-dimensional Dirichlet-distribution of p11 ; p10 and p01 (p00 =1‚àíp11 ‚àíp10 ‚àíp01 )
in the hyperparameters 11 ; 10 ; 01 ; 00 . Consider, e.g. the distribution of P11 [P(p11 )]:
P(p11 ) = Di(p11 |
=

11 ; 10 ; 01 ; 00 )

( 11 +
( 11 ) (

+
10 ) (
10

+ 00 ) 11 ‚àí1 10 ‚àí1 01 ‚àí1
p11 p10 p01 (1 ‚àí p11 ‚àí p10 ‚àí p01 ) 00 ‚àí1 :
01 ) ( 00 )
(20)

01

478

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

The marginal distributions to Dirichlet are also Dirichlet but in this case they reduce
to a one-dimensional Dirichlet which is a Beta (14). The posterior distribution given
the counters c11 ; c10 ; c01 ; c00 is also a Dirichlet distribution (Bernado and Smith, 1994)
P(p11 |c11 ; c10 ; c01 ; c00 ) = Di(p11 |c11 +

11 ; c10

+

10 ; c01

+

01 ; c00

+

00 ):

The expectation value E(p11 ) thus becomes:
E(p11 )

R1R1R1

p11 Di(p11 |c11 + 11 ; c10 + 10 ; c01 + 01 ; c00 + 00 ) dp01 dp10 dp11
= 0R 10R 10R 1
:
Di(p
|c
+
;
c
+
;
c
+
;
c
+
)
dp
dp
dp
11
11
11
10
10
01
01
00
00
01
10
11
0 0 0
The evaluation of this integral involves some hyper-geometric functions and is a
bit cumbersome and we skip the details here. These expectation values can also be
looked up in a statistical textbook like Bernado and Smith (1994). We end up with
the following:
E(p11 ) =

c11 + 11
c11 + 11 + c10 + 10 + c01 +

01

+ c00 +

00

=

c11 +
C+

11

(21)

and for the variance (observe similarity with (18), (19))
V (p11 ) =
=

E(p11 )(1 ‚àí E(p11 ))
1 + c11 + 11 + c10 + 10 + c01 + 01 + c00 +
(c11 + 11 )(C + ‚àí c11 ‚àí
(C + )2 (1 + C + )

11 )

00

:

(22)

2.2. Weights and information components
In our rst attempt to nd the expectation values for the weights [E(Wij ) =
E(pij =pi pj )] and their variances we tried the same approach as above by using
the integral:
Z
0

1

Z
0

1

Z
0

1

p1111 ‚àí1 p1010 ‚àí1 p0101 ‚àí1 (1 ‚àí p11 ‚àí p10 ‚àí p01 ) 00 ‚àí1
dp11 dp10 dp01 :
(p11 + p10 ) 11 + 10 ‚àí2 (p11 + p01 ) 11 + 01 ‚àí2

(23)

We could, however, not nd any closed-form solution to this, which would still not
have taken into account any cross dependencies. Instead the following approximation
is used, utilizing (18), (21) above, where
and
are the number of mutually
exclusive events in each class for the variables i and j, respectively,
E(Wij ) ‚âà

E(pij )
(cij + ij )(C + )(C + )
=
:
E(pi )E(pj )
(C + )(ci + i )(cj + j )

(24)

For the speci c case of the ICij this can, however be calculated exactly, due to
pij
E(ICij ) = E log
pi pj

!

= E(log pij ) ‚àí E(log pi ) ‚àí E(log pj )

(25)

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

479

and it can be shown (Koski and Orre, 1998) that when p is Beta(a; b) distributed,
then
E(logp) =

‚àû
X
b
1
‚àíb
:
a(a + b)
(a + n)(a + b + n)
n=1

(26)

Here a = 1 + c1 and b = 0 + c0 . In the application work we present here we have,
however, used the following simpli ed form for the expectation value E(ICij ):
E(ICij ) ‚âà log E(Wij ) ‚âà log

E(pij )
:
E(pi )E(pj )

(27)

The variance for the weight [V (Wij ) = E(Wij2 ) ‚àí E(Wij )2 ] is harder to estimate.
So far we have used
the Gauss‚Äô approximation for the variance of a function, i.e.
P
V [g(X1 ; : : : ; Xk )] ‚âà ki=k V (Xi )(@g=@i )2 , and not included covariant terms.
We assume symmetrical distributions, therefore we set i = E(Xi ). The variance
for the weight V (Wij ) then is
2
2
V (pij ) pÃÇij V (pi ) pÃÇij V (pj )
V (Wij ) ‚âà 2 2 +
+
pÃÇi pÃÇj
pÃÇ4i pÃÇ2j
pÃÇ2i pÃÇ4j

(28)



=

(C + )2 (C + )2 (cij + ij ) (C ‚àí cij + ‚àí ij )
(C + )2 (ci + i )2 (cj + j )2
(1 + C + )
#

(cij + ij )(C ‚àí ci + ‚àí i ) (cij + ij )(C ‚àí cj + ‚àí j )
+
+
:
(ci + i )(C + + 1)
(cj + i )(C + + 1)

(29)

For the information component ICij we can, due to the properties of the log-function
as in (25) and in Koski and Orre (1998) write the variance V (ICij ) as an exact
expression (here including covariant terms):
V (ICij ) = V (log pij ) + V (log pij ) + V (log pij )
‚àí2 cov(log pij ; log pi ) ‚àí 2 cov(log pij ; log pj ) + 2 cov(log pi ; log pj )
(30)
and it can be proved (Koski and Orre, 1998) that for p being Beta(a; b) distributed,
then
V (log p) =

‚àû
X
n=0

b2 + 2ab + 2bn
:
(a + n)(a + b + n)2

(31)

For V (ICij ) we intend using expression (30) with covariant terms in the future.
Although we are still using the simpler approach with Gaussian approximation, assuming independence:
1
V (ICij ) ‚âà V (pij )
pÃÇij

!2



‚àí1
+ V (pi )
pÃÇi

2

‚àí1
+ V (pj )
pÃÇj

!2

:

(32)

480

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

Here we measure the ICij in bits (i.e. use log2 ), which gives the following explicit
expression:
V (ICij )
‚âà

C‚àícij + ‚àí ij
(cij + ij )(1+C+ )

+

C‚àíci + ‚àí i
(ci + i )(1+C+ )

C‚àícj + ‚àí i
(cj + j )(1+C+ )

+

(log 2)2

:

(33)

2.3. Variance of conditioned posterior distribution
To calculate the variance of P(aj |D), below, we do a logarithmic exponential
transformation, using Gaussian approximation for variance of a function [V (g(X )) ‚âà
V (X )((g=X )(E(X )))2 ] thus V (X ) = V (elog[X ] ) ‚âà V (log[X ])E(X )2 . An approximate
variance for a sum of independent
terms,
(38), can then be calculated using the
P
P 2
Gaussian approximation V ( i ci Xi ) ‚âà i ci V (Xi ).
Let (aj |D) below, be the expression after the
independence assumption (4). When
P
1=
=
1=
(aj |D) is scaled by the coecient
j (aj |D), we obtain the expression
P
for the posterior P(aj |D), i.e. j P(aj |D) = 1. From Eqs. (3) and (6) we get
P(aj )P(D|aj )
(aj |D)
(aj |D)
;
P(aj |D) = P
=P
=

j P(aj )P(D|aj )
j (aj |D)
j = (aj |D) = P(aj )

Y X P(dk ; aj )
i

;

i

=

X

k

P(dki )P(aj )

(34)

dki ;

(35)

(aj |D):

(36)

j

By using a Taylor expansion the variance V (P(aj |D)) can be approximated as
V (j )
cov(j ; )E(j ) V ()E(j )2
‚àí
2
+
:
(37)
E()2
E()3
E()4
The variance V () is zero, due to  being a function of the applied data pattern
only (3). For a similar reason the covariance cov(j ; ) is also zero, as we only
consider dki to be a random variable during the training phase of the network. In (38),
below, we start with V (log[(aj |D)]). We set [Wijk =P(dki ; aj )=P(dki )P(aj )] in (39) and
P
consider log[P(aj )] and k Wijk dki to be independent in (40). A logarithmic variance
transformation is performed from (40) to (41). The dki represents part of a mixture of
k belief values, which are here coecients only, without a variance. As dki is part of
P
P
a mixture [ k dki = 1] it is reasonable to assume that 2 k¬°l ik il cov(Wijk ; Wijl ) ‚â§ 0.
The Gaussian approximation for a sum of independent variables will then be a worst
case estimate of (41), which results in inequality (42):
V (P(aj |D)) ‚âà

V (log[(aj |D)]) = V

log[P(aj )] +

X

"

log

i

=V

log[P(aj )] +

X
i

"

log

X P(dk ; aj )
i
k

X
k

P(dki )P(aj )

#!

dki

(38)

#!

Wijk dki

(39)

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

X

‚âà V (log[P(aj )]) +

"

V

log

i

X
k

#!

Wijk dki

(40)

P
X V ( k Wijk dk )
i
P
‚âà V (log[P(aj )]) +
k
2

E(

i

X



¬° V log[P(aj )] +

i

k

P
k

E(

(41)

Wij dki )

V (Wijk )d2 k

P

481

i

k
2
k Wij dki )

:

(42)

When we know that dki represents
k mutually exclusive discrete inputs, then we
P
can rewrite (40) as (43), because k is then a sumPover one single value only, as
all other values are zero. Therefore, we can move k and dki outside the variance
expression, which is done in (44). The [log Wijk ] we have earlier named the information component [ICijk ] (10) and for that we have an exact variance expression (30)
and for V (log[P(aj )]) as well (31), which gives Eq. (45):
V (log[(aj |D)]) ‚âà V (log[P(aj )]) +

X

V

i

‚âà V (log[P(aj )]) +
= V (log[P(aj )]) +

k

XX
i

k

XX
i

X

k

!

log[Wijk ]dki

(43)

V (log[Wijk ])dki

(44)

V (ICijk )dki :

(45)

Using either (42) or (45), depending on the type of input events, we can calculate
V (log((aj |D))). The variance V ((aj |D)) thus becomes
V ((aj |D)) = V (exp(log[(aj |D)])) ‚âà V (log (aj |D))E((aj |D))2 :

(46)

Expression (37) for the variance V (P(aj |D)) can thus be written
V (P(aj |D)) ‚âà V (log[(aj |D)])

E((aj |D))2
P
:
E( j (aj |D))2

(47)

2.4. The selection of reasonable priors
The priors for pi and pj are not critical as the convergence to the ‚Äúreal probability‚Äù
is rather quick Bernardo and Smith, 1994. The most simple prior for a binary variable
to use here is 1 = 0 = 1, i.e. a non-informative (sometimes called ignorant prior)
Bernado and Smith, 1994, which corresponds to an a priori assumption about equal
probability distribution. We should, however, be aware that for a non-binary discrete
variable with k states, we need another prior where each speci c state of the variable
is considered. We could then assert for example i = 1; = k i ; j = 1; = l j . To
get a coherent prior for pij we would then set ij = 1; = . We have, however,
chosen to make a slight drawback from the coherence criterion and instead chosen
a prior for pij that behaves well for small counter values in the way that when we

482

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

Fig. 1. Some examples of prior distributions for P(pi ) and P(pij ) (brie y Pi and Pij in the diagrams
above). The upper diagrams show the priors for P(pi ) when [ 0 = 1 =1] (c0 =0; c1 =0); (c0 =2; c1 =1)
and (c0 = 14; c1 = 1) respectively. The lower diagrams show the corresponding estimated priors for
P(p11 ) when [ 11 = 1] c11 = 0; c11 = 1; c11 = 1 when both i; j are de ned as above.

have no data samples of pairs of variables we consider them to be independent (48)
and then choose the prior for pij so that (49) is ful lled.
lim

ci ;cj ;cij ‚Üí0

ICij = log

lim ICij = log

cij ;C‚Üí0

pÃÇij
‚âà 0;
pÃÇi pÃÇj

pÃÇij
(cij + ij )=(C + )
= log
‚âà 0:
pÃÇi pÃÇj
pÃÇi pÃÇj

(48)

(49)

Then we can set ij = 1 and = ij = pÃÇi pÃÇj .
In Fig. 1 we see an example of how this looks when starting with the ignorant
prior and then how the posterior distribution gets more and more narrow when we
add a few samples. In the gure it is also shown how the estimated prior for the
joint distribution Pij look like for some of the rst samples.

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

483

3. Other method related issues
3.1. Variable value coding
The coding of binary and discrete variables into a neural layer representation is
rather straight forward. For a binary variable x we input the values [x; x ]. This is
also the simplest example of a hypercolumn, a neural layer with mutually exclusive
input units. When a variable value is missing we may input the a priori probabilities
[px ; px], alternatively we code it as a default value. Any discrete variable, whose
values are mutually exclusive is coded in this way. In general, we may input a
normalized mixture of belief values instead of the a priori probabilities for missing
values. Real-valued variables are coded using such a mixture of belief values, which
represents the degree of membership to a set of radial basis functions (RBFs). The
placement of these RBFs is usually done using the EM algorithm (Tra ven, 1993)
3.2. Dependent variables
Variables which are found to be dependent on each other can be handled by
coding the combination of states for these variables into a separate subspace, a
hypercolumn, a neural layer where all combinations are mutually exclusive. As an
example, assuming that the binary variables X and Y are dependent on each other,
we make a hypercolumn with 4 units representing {xy;
 xy;
 xy;
 xy}. When the number
of combinations get large this coding may be inconvenient, then a reduced coding
is used, where only the combinations or features that actually occur in training data
are coded. Real-valued variables are handled in a similar way. The RBF units, which
are then used, may combine an arbitrary number of dimensions into, which can be
seen as, a normalized mixture of belief values as one hypercolumn. The RBF coding
is done ‚Äúbefore‚Äù the treatment of discrete variables, which is necessary to be able to
combine real-valued variables with discrete variables. To nd dependencies between
variables we use the following methods ( is a threshold parameter):
‚Ä¢ Check for strong pairwise mutual information between all pairs of subspaces. The
procedure is repeated to nd higher order combinations as long as
MI ¬°

X

pxy log

xy

pxy
:
px py

(50)

‚Ä¢ Check all variable combinations up to a certain complexity level in one shot. To
decide what combinations to save we use the Kullback‚ÄìLeibler (Bishop, 1995)
distance between the joint distribution and the marginal distributions. We save
those subspaces where
KL3::: ¬°

X
xyz:::

pxyz::: log

pxyz:::
:
px py pz : : :

(51)

‚Ä¢ Check all combinations of variables up to a certain complexity level in one run.
Save those combinations only, where the information component between input

484

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

and output layer is above a certain threshold IC which is
pxy
IC ¬° log
:
(52)
px py
A comment on the thresholds MI ; KL and IC above: At the moment these kind
of thresholds are considered to be design parameters in the BCPNN network. We
have no automatic method for generating the threshold levels yet.
3.3. Sparse matrix technique
When working with this huge WHO database, of adverse drug reactions, we rst
used full matrixes. We found this to be inecient because a typical full connection matrix, containing 20 ‚Äì50 million connection elements often contained a non
zero value in 1‚Äì2% of the positions only. Therefore we developed a sparse matrix
technique, which reduced both the required computer time as well as memory requirements drastically as it does not create matrix elements until they are needed.
The technique is ‚Äúdouble sparse‚Äù, i.e. it allows us to create not only matrix elements
dynamically, but also the ‚Äúneurons‚Äù in the input=output layers dynamically.
Owing to this sparse technique and the organization of the database in reports,
where only a very small subset of all possible combinations can occur on each, we
do not need to do the search for dependent variables completely incrementally. We
can decide beforehand how high a complexity level of combinations we want to
investigate.
4. Results and examples
4.1. Signal generation
In the database application we want to generate an early warning signal when a
certain dependency between a drug or a set of drugs and an adverse drug reaction
(ADR) or a set of ADRs is detected. The procedure is to look for signi cant di erences in weight values between input and output variables when a batch of reports
is added to the database. To be able to do this in an ecient manner from the
perspective of computing time and memory we used a sparse matrix coding of the
connection matrix. The procedure was tested on some well-known signals like the
association between the drug suprofen and back pain, and azapropazone photosensitivity. Results from these time scans are shown in Figs. 2‚Äì5.
In these experiments the BCPNN network was set up in the normal way, i.e. C is
the total number of reports in the database, ci is the number of reports for the drug,
cj is the number of reports for the adverse reaction and cij is the number of reports
where the drug and the adverse reaction occur on the same report.
The diagram in Fig. 2 shows how the information component (IC) for the suprofenback pain association varies between the years 1983 and 1990. The bars around the
IC curve show, for each quarterly year on the x-axis, a 95% con dence interval for
the IC. The diagram in Fig. 3 shows how the cumulative probability function for

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

485

Fig. 2. A well-known signal: suprofen and back pain. The diagram shows the IC (information component) for the drug-ADR association. The error bars show a 95% estimated conf. interval.

Fig. 3. Suprofen and back pain: The diagram shows how the P(IC ¬ø 0) develops over time, we see a
clear indication of this association with 80% certainty already after the rst quarter 1984.

Fig. 4. The development from 1973 to 1990 of the information component for the drug azapropazone
vs the photosensitivity reaction with 95% conf. int.

486

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

Fig. 5. Logarithmic scale, with 95% conf. int. Priors: P(i) = P(azapropazone), P(j)=
P(photosensitivity). Posterior: P(j|i) = P(photosensitivity|azapropazone).

R‚àû

IC being greater than zero [P(IC ¬ø 0) = y=0 Py (IC) dy] develops over time. A case
report of acute ank pain after taking 3 doses of suprofen was rst published in
1986 (Henann and Morales, 1986).
From the diagrams in Figs. 2 and 3 we can see indications of an association
between the drug and the adverse reaction with rather high certainty, around 80%
after the rst quarter 1984, which rises to around 97% in the middle of 1984, when
we would signal it. The current criterion for the detection of a signal is when the
lower 95% con dence limit of the IC for the drug-ADR combinations changes from
a negative to a positive value on addition of the data for the last quarter.
For the azapropazone case there was a paper published in 1985 of this drug being
associated with photosensitivity reaction Olsson et al., 1985. The diagram in Fig. 4,
which shows the IC for azapropazone vs photosensitivity reaction, indicates this association would be highlighted with this approach in 1975. In Fig. 5 we see the prior
probabilities for the drug and the adverse reaction. Observe that the scale for the
probabilities in this diagram is logarithmic. We also see the posterior probability for
the adverse reaction given the drug. As can be seen P(A|D)P(A), which clearly
indicates a conditioned dependency between the drug and the ADR. All three probabilities are shown with 95% con dence intervals, but the prior probabilities are much
narrower than the conditioned posterior probability because there are less samples in
the joint distribution.
4.2. Digoxin versus age and rash
The following experiments aim to demonstrate that IC analysis can be used to
study the relationship between combinations of any variables in the database, including, but not being restricted to, drug adverse reaction association pairs. To establish
this, the relationship between the drug digoxin and the patient‚Äôs age was examined
by observing the change in the IC for the association digoxin-rash over di erent age
intervals. Also, the strength of association between digoxin and age intervals was
examined for the entire database.

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

487

Fig. 6. A time scan of IC for the drug digoxin vs the ADR rash from the year 1967 to 1997. At 1997
the IC has stabilized around a level of ‚àí2.

The BCPNN network was here set up to generate counters in a slightly di erent
manner than previously. C would normally be the total number of reports in the
database, but in two of these experiments (in Figs. 8 and 9) it was set as C=total
number of reports within the speci c age group under consideration. The age grouping used here are 10 year intervals. The ci is the counter for the drug or for the
drug combined with age. The occurrences of the drug being reported as ‚Äúsuspected‚Äù
(Fig. 8) or concomitant medication (‚Äúother‚Äù) (Fig. 9) are counted separately. cj =
the number of reports for the adverse reaction or the adverse reaction combined with
age group, and cij =counts the intersection between ci and cj in the normal way.
In Fig. 6 we see a normal time scan of the IC for digoxin vs. the adverse reaction
Rash from 1967 to 1997, when the IC has stabilized at a level of ‚àí2. The diagram
in Fig. 8 shows the IC for the database up to the end of 1997, but here displayed
separately for di erent age groups. In this diagram (Fig. 8) we see, for each age
interval, that there was a negative IC between digoxin and rash. The association
was most negative for age range 30 ‚Äì 40, although in general there seemed to be a
trend towards lower ICs for higher ages, i.e. less probability for digoxin to be the
suspected drug for causing Rash in elderly patients. However, the con dence intervals
are rather large and the trend is therefore unreliable, based on the data available at
the time. We can also see that the uncertainty in IC is higher for younger patients,
which may be explained by the diagram in Fig. 7, where we see how the IC for
digoxin vs. age varies with the age of the patient. The diagram of IC vs. age in
Fig. 7 shows a clear trend of increasing IC with age. From a minimum of IC = ‚àí4
for 20 ‚Äì30 year olds (cij = 34) to a maximum value of IC = 3 (cij = 244) for the
age group of 90+ year olds. The highest cij value was for 70 ‚Äì80 year old patients
where (cij = 2228) (IC = 1:7). The standard deviations are small for all IC values
due to the large number of reports of digoxin in the database (7370) (Fig. 8).
In Fig. 9 the IC between digoxin and Rash within di erent age groups is shown
when digoxin was not the suspected drug but was reported as concomitant medication. In the same way as for the results where digoxin was the suspected drug most
digoxin-rash associations had negative ICs , however there was a de nite trend of

488

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

Fig. 7. Here we see how the association between digoxin and patient age varies with the age of the
patient. Thus indicating a higher probability to nd elderly patients being reported with digoxin than
younger patients.

Fig. 8. IC between digoxin and rash for the last quarter 1997 displayed separately for each age group,
with 10 year intervals. The age group ‚Äúall‚Äù sums all intervals.

increasing positive IC for increasing age range, so that for age groups 70 ‚Äì80, 80 ‚Äì90
and 90+ there was a de nite positive association between digoxin, when recorded
as concomitant medication, and rash.
4.2.1. Discussion
The contrast between digoxin‚Äìrash pro les over age for digoxin as a suspected
drug and digoxin as concomitant medication was striking. This is probably because
when elderly patients are take digoxin they are more likely to be taking other drugs
concomitantly, than a younger person taking digoxin. Therefore, the occurrence of
rash as an adverse reaction is more likely to be attributed to another drug, for
elderly patients as compared to younger patients. This experiment shows that the
BCPNN methodology can be used to look at associations when 3 di erent variables
are considered together.

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

489

Fig. 9. The IC between digoxin and rash displayed separately for each age group, last quarter 1997.
This concerns when digoxin was reported as concomitant medication (‚Äúother‚Äù).

5. Search for dependent variables
5.1. Goal description
The BCPNN methodology can be used to search for dependent variable combinations of any order. Here we provide an example using this method to data mine
the database to assess the validity of our complex variable method and to provide
an indication of the e ectiveness of the methodology in nding combined variable
e ects. In this example, a syndrome, which is a group of concurrent symptoms is
investigated. This experiment also shows that the method is computationally tractable.
We considered a known adverse drug reaction syndrome complex association:
Neuroleptic Malignant Syndrome (NMS) which is frequently reported in the WHO
database, and is mainly associated with antipsychotic drugs. The syndrome itself is
a combination of several symptoms which themselves can be reported as individual
adverse drug reactions. The following adverse reactions were selected as indicators:
Creatine Phosphokinase Increased, Fever, Death and Hypertonia. Although death is
an outcome it was included because it is also a ‚Äúreportable‚Äù term in the adverse
reaction terminology.
We are interested therefore in the associations between combinations of these
adverse reactions with haloperidol, an antipsychotic drug known to cause NMS, and
how the strength of the associations with the combinations compare with the strength
of associations with the adverse reactions themselves. Ultimately we wish to know
whether it will be possible to pick the syndrome out, not knowing the constituent
reactions beforehand.
5.2. Setup of the experiment
We aimed to investigate all single, pair and triplet combinations of all adverse
reactions in the database and then examine the strength of the association of each

490

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

combination with the drug haloperidol. For this purpose we used the sparse matrix
(see Section 3.3) method which was an appropriate tractable method in this case.
One conventional method to use would be to rst scan the database to check all
ADRs against all ADRs, then make a selection of what ADR pairs to consider based
on some threshold. After this the database could be rescanned checking these selected
ADR pairs vs. all ADRs again. Then a new selection could be made by thresholding
and these triplets checked vs. the drug.
This unsupervised approach works well in nding general feature detectors as it
would, in most cases, nd combinations where the Kullback‚ÄìLeibler distance between
the joint adverse reactions density and the marginal product density
X

p(ADR123 )log

123

p(ADR123 )
p(ADR1 )p(ADR2 )p(ADR3 )

(53)

would be quite large, but this would not necessarily give us all the reactions we
really want to nd, i.e. all combinations where
p(ADR123 |drug)p(ADR123 ):

(54)

The speci cation given the sparse BCPNN was to partition the drugs into the classes
‚Äúhaloperidol‚Äù, ‚Äúother drug‚Äù as input layer and make all possible combinations of adverse reactions in the output layer, i.e. the output layer will represent a subset of
the power set of all ADRs on each report. The subset we used here included combinations of up to three ADRs. This would also allow us the to check, e.g. the
KL-distance (53) for all ADRs found in the database at once, which gave us a
tremendous speedup compared with the original matrix approach, which took several
days on a Sun UltraSparc. The actual search needed only about 7 h of computing
time on the same UltraSparc. At the same time we are able to consider all possible combinations of triplets of ADRs in the database to nd those that satisfy:
p(ADR123 |drug)p(ADR123 ).
5.3. Results
We generated lists of the associations according to the following table:
Drug

ADR-comb

# ADR comb

# IC ¬ø 0

Haloperidol
Haloperidol
Haloperidol

Single-ADR
Double-ADR
Triple-ADR

1700
35 000
55 0000

281
4019
5388

where the column ‚Äú# ADR comb‚Äù tells us how many combinations that were found
in total. The column ‚Äú# IC ¬ø 0‚Äù tells us how many of these had a positive IC. The
ones with a positive IC were then sorted on the level of the IC, i.e. the strength of
the association between the drug and the ADR-combination.
As was expected the term NMS was on the top of all these lists. In the pairs and
triplets list NMS was also found to be strongly associated with some of the other

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

491

symptoms which are included in the symptom picture of the selected ADR terms.
We also found that the selected ADRs where high on all three lists. For the single
ADR list all four were among the highest 200 IC values. For the list with ADR pairs
combinations of these four ADRs were also found among the highest 200 IC values,
and three of these were in the top 10. For the list with triple ADRs all combinations
with these four ADRs were among the highest 400 and three combinations were in
the top 10.
6. Discussion
This paper presents a new ecient methodology for data mining of large databases
in a computationally feasible way. The method not only provides a way to calculate
conditional dependencies and predictive posterior probabilities within the data, but
also estimates of the variances of the corresponding distributions, which makes this
method accurate also for small sample set sizes of the investigated variables within
the data set. Although the method has been demonstrated on a speci c database it
is suitable for other data mining applications.
We have made extensive use of the Gaussian approximation formula for the variances of functions here. These approximations are best suited for Gaussian distributions, although here we use Beta and Dirichlet as our model distributions. To allow
for this, particularly when the conditional independence assumption between input
variables is not ful lled, we code the dependent variables into hypercolumns, that
is, partition the input space into mutually exclusive regions. Further on, to make the
calculations simpler, we do not yet consider covariance terms in the calculations.
Initially, it was dicult to do exact calculations of the expectation value and the
variance of the ICij . It was therefore encouraging to nd, as described in Section
2.2 and also in Koski and Orre (1998), that using the logarithmic form of the ICij
we can express these solutions in an exact analytical way. This is being considered
in ongoing work.
In the results presented in this paper we have propagated probabilities and calculated the variances of posterior output distributions conditioned on a set of inputs by
approximating the Wij . We expect that the use of the analytical expression for the
ICij will help us to make a better approximation of Wij .
We intend to investigate the possibility of nding an exact expression for the
conditioned output probability distribution, or at least, its variance. Alternatively we
could approximate these variances reasonably by the use of numerical integration.
This would, however, result in a very large increase in the computational power requirement. This can certainly be done if necessary and we will consider this approach
in the future.
Although the neural network technology we use is not only computationally but
also architecturally ecient other methodological approaches may be similarly applied to these variance calculations. There are statistical methods being developed
that may do better in approximating the variances like ‚Äúsaddle point approximations
for statistical series‚Äù (Kolassa, 1997), which may be computationally feasible, but

492

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

we have not been considered these yet. Our goal is to be able to propagate complete
distributions and for this purpose sampling techniques, like Gibbs sampling, are often
used today. This kind of technique is, however, at the moment, too computationally
demanding to be used in our data mining application.
We believe that our approach provides a mechanism for earlier and more ecient
signalling of suspected adverse drug reactions. This application is dealt with in more
detail in Bate et al. (1998). The method is now (spring 1998) being used to produce
warning signals on new unexpected drug adverse reaction associations when they
become signi cant in the database.
Acknowledgements
We gratefully acknowledge the reporting of spontaneous adverse drug reaction
case reports from National Centres to the WHO Collaborating Centre for International Drug Monitoring, which make up the database described in this paper.
Rogelio Melhado Defretias was the person that initialized the contact between WHO
Uppsala monitoring centre and Royal Institute of Technology. Thanks to Anders
Holst who supplied a lot of patience with us discussing the issues herein. He also
provided the original idea of how to do the variance calculations using Gauss formula. Thanks to Stefan Arnborg for valuable comments.
References
Bate, A., Lindquist, M., Edwards, I.R., Olsson, S., Orre, R., Lansner, A., De Freitas, R.M., 1998. A
bayesian neural network method for adverse drug reaction signal generation. Eur. J. Clin. Pharmacol.
54, 315‚Äì321.
Bayes, T., 1763. An essay towards solving a problem in the doctrine of chances. Biometrika (reprint
1958 of orig art in Philos. Trans. Roy. Soc. London 53, pp. 370 ‚Äì 418) 45, 296 ‚Äì315.
Bernado, J.M., Smith, A.F.M., 1994. Bayesian Theory. Wiley, Chichester.
Bishop, C.M., 1995. Neural Networks for Pattern Recognition. Oxford University Press, Oxford.
Good, I.J., 1950. Probability and the Weighing of Evidence. Charles Grin, London.
Heckerman, D., 1997. Bayesian networks for data mining. Data Minining Knowledge Discovery 1,
79‚Äì119.
Henann, N.E., Morales, J.R., 1986. Suprofen - induced acute renal failure. Drug Intell. Clin. Pharm 20
(11), 860‚Äì862.
Holst, A., Lansner, A., 1995. A higher order bayesian neural network for classi cation and diagnosis.
In: Applied Decision Technologies: Computational Learning and Probabilistic Reasoning. London,
England, April 3‚Äì5, pp. 251‚Äì260.
Koski, T., Orre, R., 1998. Statistics of the information component in bayesian neural networks.
Technical Report of TRITA-NA-9806, Department of Numerical Analysis and Computing Science.
Royal Institute of Technology, Stockholm, Sweden.
Kolassa, J.E., 1997. Series Approximation Methods in Statistics. Springer, Berlin.
Kononenko, I., 1989. Bayesian neural networks. Biol. Cybernet. 61, 361‚Äì370.
 1985. Reliability and speed of recall in an associative network. IEEE Trans.
Lansner, A., Ekeberg, O.,
Pattern Anal. Mach. Intell. 7 (4), 490 ‚Äì 498.
 1989. A one-layer feedback arti cial neural network with a bayesian learning
Lansner, A., Ekeberg, O.,
rule. Int. J. Neural Syst. 1 (1), 77‚Äì87.

R. Orre et al. / Computational Statistics & Data Analysis 34 (2000) 473‚Äì493

493

Lansner, A., Holst, A., 1996. A higher order Bayesian neural network with spiking units. Int. J. Neural
Syst. 7 (2), 115‚Äì128.
Laplace, P.S., 1825. A Philosophical Essay on Probabilities, Trans 1995 from the 5th Edition. (orig
1814) by Dale, A., 1814. Orig ‚ÄúEssai Philisophique sur les Probabilites‚Äù. Dover Publications, New
York.
Olsson, S., Biriell, C., Boman, G., 1985. Photosensitivity during treatment with azapropazone. Br. Med.
J. 939.
Orre, R., Lansner, A., 1996. Pulp quality modelling using bayesian mixture density neural networks.
J. Syst. Eng. 6, 128‚Äì136.
Pearl, J., 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan
Kaufmann, San Mateo.
Tra ven, H.G.C., 1993. On pattern recognition applications of arti cial neural networks. Ph.D.
Thesis, Royal Institute of Technology, Department of Numerical Analysis and Computing Science.
Stockholm, Sweden.

