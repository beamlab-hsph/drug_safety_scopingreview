Drug Safety (2019) 42:721–725
https://doi.org/10.1007/s40264-018-00794-y

SHORT COMMUNICATION

A Machine‑Learning Algorithm to Optimise Automated Adverse Drug
Reaction Detection from Clinical Coding
Christopher McMaster1,2

· David Liew1,2 · Claire Keith3 · Parnaz Aminian3 · Albert Frauman1,2

Published online: 6 February 2019
© Springer Nature Switzerland AG 2019

Abstract
Introduction Adverse drug reaction (ADR) detection in hospitals is heavily reliant on spontaneous reporting by clinical staff,
with studies in the literature pointing to high rates of underreporting [1]. International Classification of Diseases, 10th Revision
(ICD-10) codes have been used in epidemiological studies of ADRs and offer the potential for automated ADR detection systems.
Objective The aim of this study was to develop an automated ADR detection system based on ICD-10 codes, using machinelearning algorithms to improve accuracy and efficiency.
Methods For a 12-month period from December 2016 to November 2017, every inpatient episode receiving an ICD-10 code
in the range Y40.0–Y59.9 (ADR code) was flagged for review as a potential ADR. Each flagged admission was assessed by an
expert pharmacist and, if needed, reviewed at regular ADR committee meetings. For each report, a determination was made
about ADR probability and severity. The dataset was randomly split into training and test sets. A machine-learning model
using the random forest algorithm was developed on the training set to discriminate between true and false ADR reports. The
model was then applied to the test set to assess accuracy using the area under the receiver operating characteristic (AUC).
Results In the study period, 2917 Y40.0–Y59.9 codes were applied to admissions, resulting in 245 ADR reports after review.
These 245 reports accounted for 44.5% of all ADR reporting in our hospital in the study period. A random forest model built
on the training set was able to discriminate between true and false reports on the test set with an AUC of 0.803.
Conclusions Automated ADR detection using ICD-10 coding significantly improved ADR detection in the study period,
with improved discrimination between true and false reports by applying a machine-learning model.
Key Points
Adverse drug reactions in hospitals are chronically
underreported via spontaneous reporting mechanisms.
Automated detection of ADRs using International Classification of Diseases, 10th Revision (ICD-10) coding
captures many ADRs missed by spontaneous reporting.
Although ICD-10 codes produce many false-positive
ADR reports, this can be reduced with machine-learning
algorithms.
* Christopher McMaster
christopher.mcmaster@austin.org.au
1

Department of Clinical Pharmacology, Austin Health,
Level 5, Lance Townsend Building, Studley Rd, Heidelberg,
VIC 3084, Australia

2

Department of Medicine, University of Melbourne, Parkville,
VIC, Australia

3

Department of Pharmacy, Austin Health, Heidelberg, VIC,
Australia

1 Introduction
Adverse drug reactions (ADRs) are cases of drug-related
morbidity occurring at therapeutic doses, in which there
is an established causal relationship between the drug and
the injury, with some nominal probability assigned to the
strength of evidence about that causal relationship [1]. Conventional ADR reporting is largely dependent on spontaneous, non-automated reporting by individuals. While ADRs
may be directly reported to pharmacovigilance databases by
consumers or health professionals, within-hospital events
may first undergo review processes to determine the severity
and causality of drug–event pairs. This is done in the interests of accuracy and governance, often aided by validated
scoring systems such as the Naranjo score [2], an algorithm
for assessing the causality of a drug–event pair, using factors
such as the temporal relationship, resolution on drug discontinuation and recurrence with drug restarting. The severity and plausibility of an ADR report subsequently informs
decisions about on-reporting to external pharmacovigilance
databases, and necessary actions for the individual patient.

Vol.:(0123456789)

722

This can include recording an ADR on the patient’s medical
record, plus providing a written summary to the patient and
their general practitioner.
Despite the clinical impetus to report ADRs for hospitalised patients, studies of reporting rates have consistently
found ADR underreporting exceeding 80% and up to 98%
[3]. While more serious ADRs are better detected, they are
still highly underreported and this can have devastating
consequences if repeat exposure is not prevented in the initially affected individual or other susceptible patients. For
the community at large, failure to adequately capture ADRs
can significantly undermine pharmacovigilance efforts and
may delay or prevent the identification of important safety
signals requiring regulatory action. This has become even
more pertinent in the era of expedited drug approvals by
regulators.
The barriers to spontaneous ADR reporting have been
studied extensively, with non-expert clinicians and pharmacists working in hospital wards and clinics and consistently
pointing to the common factors of insufficient incentive, time
and knowledge [4, 5]. Automated detection systems offer
the possibility of enhanced ADR reporting, with increased
report generation and centralisation of the human element in
the hands of domain experts. In related work, several groups
have developed algorithms to predict drug-related morbidity
in hospitalised patients, however a recent systematic review
found most to have significant statistical and methodological
flaws [6]. Specifically, no algorithm has been implemented
to accurately predict or report ADRs, the specific subset of
drug-related morbidity of most interest.
Numerous epidemiological studies have used International Classification of Diseases, 10 Revision (ICD-10)
codes to automatically capture drug-related morbidity [7].
The ICD-10 codes from Y40.0 to Y59.9 comprise 175 medication-related external injury codes, which are intended to
code for cases in which a correct drug properly administered
in therapeutic or prophylactic dosage is the cause of any
adverse effect [8]. These codes have commonly been used
to examine adverse drug event (ADE) prevalence among
hospitalised patients [7], including in the Australian setting
[9]. The strength of ICD-10 coding as a pharmacovigilance
tool is its ubiquity as a clinical dataset; however, these codes
have not been validated for their ability to capture causal
drug–event pairs (ADRs).
We therefore established an automated drug-related morbidity reporting mechanism using admission ICD-10 codes.
We examined each event for severity and causality to define
a subset of verified ADRs. Using this dataset, we trained
machine-learning models to discriminate the ADR subset
of events from non-ADR events.

C. McMaster et al.

2 Methods
2.1 Institutional Setting
ADR reports were collected from a metropolitan tertiary
teaching hospital network in Melbourne, Australia. The 900bed network provides general and subspecialty medical and
surgical care to predominantly adult patients, and includes a
large cancer centre, as well as state-wide referral services for
liver transplants, toxicology and spinal cord injuries.

2.2 Constructing Training and Test Sets
Consistent with standard practice in Australia, every emergency department and inpatient episode in our health network is routinely reviewed by professional clinical coders
using the ICD-10 coding system for funding purposes.
For a 12-month period from December 2016 to November
2017 inclusive, every episode that received a code in the
Y40.0–Y59.9 range (indicating potential ADRs) was flagged
for review as a potential ADR. Additional data that were
automatically captured in a table were the primary diagnosis code (ICD-10 code of the primary reason for hospitalisation), patient unique identifier, patient age, patient sex,
admission date and discharge date. Where applicable, primary diagnosis code ADE probability groupings were also
included in the dataset based on reference tables derived
by others [7]. Established spontaneous ADR reporting continued as a concurrent ADR detection mechanism throughout the study period, with total ADR reports from the two
streams compared to determine the incremental increase
from automated ADR reporting.
Each flagged admission was retrospectively assessed by an
expert pharmacist, using the data recorded on the electronic
medical record (EMR), including pathology results, radiology
reports and daily clinical notes. Where necessary, contact with
the treating team was sought for further clarification. When
possible, a determination about ADR probability (‘doubtful’,
‘possible’, ‘probable’, ‘definite’) and severity (‘mild’, ‘moderate’, ‘severe’, ‘fatal’) was recorded, where ‘fatal’ ADRs are
those in which the ADR is likely to have contributed to the
patient’s death. If further input was required, the decision was
made at a regular ADR meeting, chaired by an experienced
clinical pharmacologist. Determinations of likelihood were
based on the Naranjo score [2], with expert physician clinical
pharmacologist opinion overriding if considered appropriate.
The models were trained to perform binary classification, with
the outcome variable taking the value 0 if ADR probability
was ‘doubtful’ and 1 for all other ADR probabilities—these
will be referred to as ‘not ADRs’ and ‘ADRs’, respectively.

723

A Machine-Learning Algorithm to Optimise Automated ADR Detection

2.3 Algorithm Training

3.2 Model

Independent variables included in the models were the
Y40.0–Y59.9 ADR code, primary diagnosis code, primary
diagnosis ADE probability grouping and a binary dummy
variable for length of stay (0 days vs. > 0 days). The dataset
was randomly split into a training set, containing 80% of the
data, and a test set of the remaining 20%. We fit L2 regularised logistic regression (otherwise known as ridge regression)
[10], support vector machine with radial basis kernel [11] and
random forest [12] classifiers with hyperparameters tuned on
the training set using a fivefold cross-validation method. To
account for imbalance in the dataset produced by the low rate
of true ADRs, down-sampling was used to produce a balanced
train set [13]. Evaluation was performed on the test set. The
models were compared for their ability to predict ADRs in the
test set using the area under the curve (AUC) of the receiver
operating characteristic (ROC). The optimal model cutpoint
was determined by Youden’s J statistic, which maximises the
linear combination of sensitivity and specificity [14].
The models were trained in R version 3.5.0 [15] using
the packages glmnet [10], e1071 [16] and randomForest
[17]. In addition to the above machine-learning algorithms,
a deep neural network (DNN) algorithm was also trained.
This algorithm had the additional input of two Bidirectional
Long Short-Term Memory type recurrent neural networks,
trained on the full text of the ADR and symptom ICD-10
codes, respectively. This model was written in the keras [18]
package, running on a TensorFlow [19] backend, trained
using a NVIDIA GTi1050 graphics processing unit. Text
embeddings were taken from the Global Vectors for Word
Representation (GloVe) 100-dimensional dataset [20].

The random forest model had superior performance on
the test set, with an AUC of 0.803 for the discrimination
of ADRs (Fig. 1). The model performance was superior
for more serious ADRs, with an AUC of 0.822 for ADRs
that were graded as severe or fatal (Fig. 2). For detecting
all ADRs, the cutpoint, as determined by Youden’s J statistic, retains 75% of the ADRs while rejecting 67.8% of
automatically generated reports. For the task of detecting
severe and fatal ADRs, the cut-point retains 80% of the
ADRs considered at least severe, while rejecting 68.4% of
all reports. Of the other models, the DNN algorithm had
the next best performance, with an AUC of 0.765 on the
test set (Fig. 1).

3 Results

Table 1  Likelihood attributions for automated and spontaneous ADR
reports

3.1 Data
In the study period, a total of 2917 Y40.0–Y59.9 codes
were applied to admissions and were therefore treated as
ADR reports. The 10 most common ADR codes accounted
for 64% of the reports. Of these ICD-10 generated reports,
it was determined that 316 were true ADRs (at least possible); 71 ADRs were indicated by two Y40.0—Y59.9
codes, therefore there were 245 unique ADR events and
thus 2672 (91.6%) of the generated reports did not result in
a true, unique ADR report. Of these 245 ADRs, 147 were
at least probable—38.4% were severe or fatal, including
a total of five fatal ADRs (Table 1). In the study period, a
total of 305 ADRs were reported through regular spontaneous reporting mechanisms, thus the ICD-10-generated
reports accounted for 44.5% of all true ADR reports in the
study period.

4 Discussion
We developed an automated ADR detection system based
on ICD-10 codes, using machine-learning algorithms to
improve accuracy and efficiency.
Automated ADR detection using minimal datasets,
such as admission coding data, has proven to be an effective way of capturing a greater number of ADEs in our
health network. These detection systems can be further
enhanced using machine-learning algorithms such as those
presented. The output of these models is a probability, thus
the automatically-generated ADR reports can be arranged
by this number. In-house ADR reviewers (pharmacists,
ADR committee members, etc.) can review reports in order
of probability, thereby allocating their limited resources to
only those most likely to represent true ADRs.

Likelihood
Doubtful/rejected
Possible
Probable
Definite
Total
Severity
Mild
Moderate
Severe
Fatal
Total

ICD-10

Spontaneous

2672 (91.6)
98 (3.4)
144 (4.9)
3 (0.1)
2917

43 (12.4)
127 (36.5)
175 (50.3)
3 (0.9)
348

27 (11.0)
124 (50.6)
89 (36.3)
5 (2.0)
245

42 (13.8)
139 (45.6)
121 (39.7)
3 (1.0)
305

Data are expressed as n (%)
ADR adverse drug reaction, ICD-10 International Classification of
Diseases, 10th Revision

724

Fig. 1  Receiver operating characteristic on test data for candidate
machine-learning models. ROC receiver operating characteristic,
AUC​area under the curve, SVM Support Vector Machine

Fig. 2  Receiver operating characteristic on test data for different
severity levels (random forest model). ROC receiver operating characteristic, AUC​area under the curve, ADR adverse drug reaction

Recent changes in drug approval pathways, including
the provisional approval pathway of the Australian Therapeutic Goods Administration [21], have created a further
need for real-world pharmacovigilance. These approval
pathways allow for regulatory approval based on limited

C. McMaster et al.

trial data, thus placing an even greater importance on postmarketing pharmacovigilance. One proposal to overcome
the gap between current pharmacovigilance practice and
future pharmacovigilance needs is the creation of a registry in which clinicians can report safety and outcome
data of patients on provisionally-approved medicines [22];
however, this does not address one of the major hurdles to
effective pharmacovigilance, spontaneous reporting. For
these provisionally-approved medicines, automated detection of ADRs may help to bridge the gap between current
pharmacovigilance practice and what is required for effective safety monitoring of such understudied therapeutics.
The proposed model carries several limitations as a
broadly applicable pharmacovigilance tool. Presently, it
has only been validated in one health network. While the
coding practices within our health network can be modelled by our machine-learning algorithms, it is uncertain
whether this can be replicated in other institutions. Additionally, our training data do not include the entire set of
Y40.0–Y59.9 and primary diagnosis ICD-10 codes. Any
entirely new codes would have to be considered as true
ADRs.
For ease of application across health services with differing informatics capabilities, a random forest model trained
on ICD-10 codes alone may be the best choice; however, for
any codes not yet seen by the model, deep learning methods
using code text has the advantage of gleaning information
from not just discrete codes but also similar word occurrences (for example, an unseen code may contain the word
‘cephalosporin’, but if the model has already seen codes
containing the word ‘penicillin’, it may infer some details
about the new code). The inclusion of other variables, such
as pathology and radiology reports, may increase model
accuracy, although the lack of uniformity of reporting and
EMR systems may make the implementation of such models
difficult.
Automated detection of ADRs using ICD-10 coding has
shown promise in improving pharmacovigilance within our
health network. Ongoing implementation will require staff
training and change to existing workflows, the impact of
which may be mitigated by the use of machine-learning
algorithms to reduce the rate of false-positive ADR notifications. Wider applicability will require external validation
and prospective implementation, but may lead to community-wide benefits for drug safety.

5 Conclusions
This single-centre, 12 month study of automated ADR
reporting demonstrated enhanced ADR detection using
Y40.0–Y59.9 ICD-10 codes, with automated reports
accounting for 44.5% of all reported ADRs in the study

A Machine-Learning Algorithm to Optimise Automated ADR Detection

period. Moreover, we were able to demonstrate the value
of a machine-learning model in automatically discriminating between true and false ADR reports generated by this
method, with a model AUC of 0.803.
Author Contributions CK: Principal Investigator. CM and CK were
responsible for the study design and conception; all authors were
responsible for acquisition and validation of the data; CM was responsible for analysis and interpretation of the data; and all authors contributed to reviewing drafts of the manuscript and approved the final
version for publication.

Compliance with Ethical Standards
Funding No sources of funding were used to assist in the preparation
of this study.
Conflict of interest Christopher McMaster, David Liew, Claire Keith,
Parnaz Aminian and Albert Frauman have no conflicts of interest that
are directly relevant to the content of this study.
Ethical approval This study was approved by the institutional Human
Research Ethics Committee.

References
1. Ackroyd-Stolarz S, Hartnell N, MacKinnon NJ. Demystifying
medication safety: making sense of the terminology. Res Soc Adm
Pharm. 2006;2(2):280–9.
2. Naranjo, et al. A method for estimating the probability of adverse
drug reactions. Clin Pharmacol Ther. 1981(2);30:661–4.
3. Hazell L, Shakir SAW. Under-reporting of adverse: a systematic
review. Drug Saf. 2006;29(5):385–96.
4. Mirbaha F, Shalviri G, Yazdizadeh B, Gholami K, Majdzadeh R.
Perceived barriers to reporting adverse drug events in hospitals: a
qualitative study using theoretical domains framework approach.
Implement Sci. 2015;10(1):1–10.
5. Bakhsh T, Al-Ghamdi M, Bawazir S, Qureshi N. Barriers, facilitators, strategies, and predictors for reporting adverse drug reactions
in three general hospitals in Jeddah, 2013. Br J Med Med Res.
2016;17(4):1–13.
6. Falconer N, Barras M, Cottrell N. Systematic review of predictive
risk models for adverse drug events in hospitalised patients. Br J
Clin Pharmacol. 2018;84(5):846–64.
7. Hohl CM, Karpov A, Reddekopp L, Stausberg J. ICD-10 codes
used to identify adverse drug events in administrative data: a systematic review. J Am Med Inform Assoc. 2014;21(3):547–57.

725
8. World Health Organization. Drugs, medicaments and biological
substances causing adverse effects in therapeutic use (Y40-Y59).
2016 [cited 28 November 2018]. http://apps.who.int/class​ifica​
tions​/icd10​/brows​e/2016/en#/Y40-Y59. Accessed 28 Nov 2018
9. Du W, Pearson S-A, Buckley N, Day C, Banks E. Diagnosisbased and external cause-based criteria to identify adverse drug
reactions in hospital ICD-coded data: application to an Australia
population-based study. Public Heal Res Pract. 2017;27(2):1–6.
10. Friedman J, Hastie T, Tibshirani R. Regularization paths for
generalized linear models via coordinate descent. J Stat Softw.
2010;33(1):128–9.
11. Karatzoglou A, Meyer D, Hornik K. Support Vector Machines in
R. J Stat Softw. 2006;15(9):28.
12. James G, Witten D, Hastie T, Tibshirani R. An introduction to
statistical learning. Berlin: Springer; 2013.
13. Kubat M, Matwin S, Rosario GE, Rundensteiner EA, Brown DC,
Ward MO, et al. Addressing the curse of imbalanced training sets:
one-sided selection. Nashville, USA; 1997. p. 179–86. https​://
arxiv​.org/pdf/1609.06570​.pdf
14. Youden WJ. Index for rating diagnostic tests. Cancer.
1950;3(1):32–5.
15. R Core Team. R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna. 2018
[cited 28 November 2018]. https​://www.R-proje​ct.org/. Accessed
28 Nov 2018
16. Meyer D, Dimitriadou E, Hornik K, Weingessel A, Leisch F.
e1071: Misc functions of the department of statistics, probability
theory group (Formerly: E1071), TU Wien. R package version
1.7-0. 2018 [cited 28 November 2018]. https​://CRAN.R-proje​
ct.org/packa​ge=e1071​. Accessed 28 Nov 2018
17. Liaw A, Wiener M. Classification and Regression by randomForest. R News. 2002;2(3):18–22.
18. Allaire J, Chollet F. keras: R Interface to ‘Keras’. R package version 2.2.4. 2018 [cited 28 November 2018]. https:​ //CRAN.R-proje​
ct.org/packa​g=keras​. Accessed 28 Nov 2018
19. Abadi M, et al. TensorFlow: large-scale machine learning on heterogeneous systems, 2015. Software available from http://tenso​
rflow​.org. Accessed 28 Nov 2018
20. Pennington J, Socher R, Manning CD. GloVe: global vectors for
word representation. 2014. https​://nlp.stanf​ord.edu/pubs/glove​
.pdf. Accessed 28 Nov 2018
21. Provisional approval pathway: prescription medicines. Therapeutic goods administration. 2018 [cited 28 November 2018]. https​
://www.tga.gov.au/provi​siona​l-appro​val-pathw​ay-presc​r ipti​onmedic​ines. Accessed 28 Nov 2018
22. Linger M, Martin J. Pharmacovigilance and expedited drug
approvals. Aust Prescr. 2018;41(2):50–3.

