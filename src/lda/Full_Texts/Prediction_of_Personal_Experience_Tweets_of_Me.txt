Prediction of Personal Experience Tweets of Medication Use via
Contextual Word Representations*
Keyuan Jiang, Tingyu Chen, Ricardo A. Calix, and Gordon R. Bernard


Abstract—Continuous monitoring the safe use of medication
is an important task in pharmacovigilance. The first-hand
experiences of medication effects come from the consumers of
the pharmaceuticals. Social media have been considered as a
possible alternative data source for gathering consumergenerated information of their experience with medications.
Identifying personal experience in social media data is a
challenging task in natural language processing. In this study,
we investigated a method of predicating personal experience
tweets using Google’s Bidirectional Encoder Representations
from Transformers (BERT) and neural networks, in which
BERT models contextually represented the tweet text. Both
pre-trained BERT models and our BERT model trained with
3.2 million unlabeled tweets were examined. Our results show
that our trained BERT model performs better than Google’s
pre-trained models (p < 0.01). This suggests that domainspecific data may contribute to the BERT model yielding better
classification performance in predicting personal experience
tweets of medication use.

I. INTRODUCTION
Continuous monitoring the safety of medication uses is an
important task of pharmacovigilance. The first-hand
experiences of medication use come from the consumers of
pharmaceutical
products.
The
consumer-generated
information is of great value in pharmacovigilance, and may
supplement the safety signal data collected through other
medication safety data sources such as spontaneous reporting
systems [1], electronic health/medical records [2], and
scientific literature [3].
In recent years, there has been a growing interest in
considering social media as an alternative source to detecting
medication safety signals, because users on social media
freely share their experiences online [4]. However, most
social media data are unstructured text with noise, making it
challenging for identification of personal experience from
such data.
Personal experience can be thought as an observed
encounter by a person. In regards to medication use, personal
experience can be any facts pertaining to the changes of a
person’s physical or health condition after intake or
administration of a medication. Understanding personal
experiences pertaining to the use of medication can help
understand effects of the medication to the subject.
*Research supported in part by the National Institutes of Health Grant
1R15LM011999-01.
K. Jiang, T. Chen, and R. A. Calix are with Purdue University
Northwest, Hammond, IN 46323 USA (corresponding author phone: 219989-2035;
fax:
219-989-3187;
e-mail:
kjiang@pnw.edu;
chen2694@pnw.edu; and rcalix@pnw.edu).
G. R. Bernard is with Vanderbilt University Medical Center, Nashville,
TN 37232 USA (e-mail: gordon.bernard@vumc.edu).

978-1-5386-1311-5/19/$31.00 ©2019 IEEE

Twitter, albeit unspecific to heath topics, is a social media
platform where users can share their personal experiences,
and has become an active source of data for possibly
detecting medication safety signals. Identifying personal
experience tweets can be considered as a classification
problem in natural language processing (NLP) in which the
task is to predict which tweet is personal experience tweet
(PET) or non-personal experience tweet (non-PET). The
following is an example of personal experience tweet (PET).
“new dose of depakote giving me really vivid bad
dreams. Ugh. I need something that will give me vivid
GOOD dreams LOL”
And below is an example of non-personal experience
tweet (non-PET).
“So celebrex can relieve arthritis but may cause stomach
bleeding, ulcers, and in some cases death. Thats crazy side
effects for medicine”
Although both tweets contain a medication name
(boldfaced) and expression(s) of effect (underscored terms),
it is important to note that the second tweet, a statement of
fact, does not describe any personal experience on the
medication (Celebrex).
Prediction of personal experience tweets is a binary
classification problem, and the traditional approach is to
engineer a set of features and then feed them into a classifier.
Contained in Twitter data are tweet text, metadata and
network information [5, 6] which are suitable for human
engineered features. However, constrained by human
knowledge and understanding, engineered features are not
necessarily the best in representing semantics of the tweet
text, resulting in sub-optimal performance of classification.
Efforts have been made in improving classification
performance of identifying personal experience tweets.
Earlier, personal pronouns were used as features to predict
personal experience tweets related to drug effects [7]. Alvaro
and colleagues [8] engineered a set of features including
Twitter specific features, n-grams, punctuation elements, and
topics, to identify first-hand experiences of prescription drug
use, but the group dropped the topic feature due to the fact
that significant efforts were required and its minimum merit
to classification performance. A set of 22 engineered features
from both textual data and metadata of tweets was considered
in constructing corpora of personal experience tweets [6].
Later, the concept of deep grammulator to enhance the
discriminatory ability of classifiers was investigated [9].
Thanks to the development in word embedding that yielded
state-of-art results in many classification tasks on textual
data, the combination of word embedding (word2vec) and

6093

Authorized licensed use limited to: Harvard Library. Downloaded on October 01,2021 at 03:41:55 UTC from IEEE Xplore. Restrictions apply.

recurrent neural network demonstrated a significant
improvement of classification performance (p < 0.01) [10].
Unlike word embedding such as word2vec which is
context free, the recently released Bidirectional Encoder
Representations from Transformers (BERT) by Google
Research is a contextual representation of words, and
generated record-breaking results in 18 different NLP tasks
[11]. Google published two pre-trained BERT models, base
and large, which can be readily used for language-based tasks
such as text classification with minimum configuration
changes. In addition, the open-source code of learning the
BERT model from the user’s own data was also made
available by Google Research, making it possible to build
custom BERT models with domain-specific data sets.

C. Data
To generate the BERT model, a large corpus of unlabeled
tweets was needed. A corpus of 22 million tweets was
retrieved with 103 medication names as query keywords with
the help of Twitter Streaming APIs which collect Tweets in
real time (https://developer.twitter.com/en/docs/tweets/filterrealtime/overview), between 25 August 2015 and 7
December 2016. The corpus was preprocessed to remove
duplicates, non-English tweets and tweets with URLs,
resulting in 3.2 million tweets for learning the BERT Twitter
model.
Figure 1. Setups of BERT models and neural networks

We investigated the performance of predicting personal
experience tweets with both pre-trained BERT models and
our own trained model from a corpus of 3.2 million unlabeled
tweets gathered with keywords of 103 medication names, and
assessed their classification performance.
II. METHOD
In this study, Google’s BERT contextual representation
models were used to represent the tweet text, and neural
networks were used as classifiers as suggested in [11]. The
setups are shown in Fig. 1.

a. One hidden layer neural network

A. Contextual Representation Models
Three different BERT models were used: BERT(base),
BERT(large), and BERT(Twitter). Both BERT(base) and
BERT(large) models were pre-trained models on the same
data sets and were provided by Google, and the major
differences between the two are that BERT(base) has 768
inputs, 12 layers, and 12 attention heads, whereas
BERT(large) has 1,024 inputs, 24 layers and 16 attention
heads.
The BERT(Twitter) model was generated with the
unlabeled Twitter data we collected using the BERT code
provided by Google Research (https://github.com/googleresearch/bert). About 3.2 million unlabeled tweets related to
103 medications were used in learning the representation, and
the model was generated with 5,000 steps of training.
The BERT model provides a special embedding output
for classification [CLS], and the embedding vector size is
different for the base model (768) and large model (1,024) –
for our own Twitter model, the size is 768 as well. This
special vector can be used as input to a classifier in the finetuning tasks such as classification [11].

b.

Three hidden layer neural network

[CLS]: a special vector output for classification
N: 1,024 for pre-trained large BERT model, and 768 for pretrained base BERT model and our BERT model trained with
tweets

For classification, a corpus of 12,331 random tweets was
constructed and annotated. The annotation was done by
following a guideline on how to annotate with examples. To
come up with the gold standard, annotators first labeled a set
of 100 tweets which were reviewed and checked by the first
author. The revised guideline was followed by two annotators
to label the rest 12,231 tweets. Upon completion, a third
annotator stepped in to resolve any disagreed labels. This
corpus of annotated tweets (tweet id and annotation) is freely
available at https://github.com/medeffects/tweet_corpora.

B. Neural Networks
There were two neural network architectures in our
research: (1) one hidden layer of 300 neurons, and (2) three
hidden layers of 100, 50, and 25 neurons, respectively (Fig.
1). The input size was dependent upon the BERT model
used. For BERT(base) and BERT(Twitter), it is 768, and for
BERT(large), it is 1,024. Sigmoid was used in the output
layer. Neural networks were implemented with the Keras
Library (https://keras.io/), a Python deep learning library. The
implementation was configured to have overfitting avoidance
and class weight adjustment for the class-imbalanced corpus
of tweets.

TABLE I.
Method
BERT(base1)a
BERT(base3)b
BERT(large1)c
BERT(large3)d
BERT(Twitter1)e
BERT(Twitter3)f

CLASSIFICATION PERFORMANCE.
Acc.
0.818
0.812
0.813
0.809
0.816
0.809

Prec.
0.619
0.594
0.607
0.592
0.594
0.576

Recall
0.634
0.691
0.631
0.662
0.774
0.792

F1
0.626
0.638
0.618
0.625
0.670
0.666

ROC
0.755
0.770
0.751
0.759
0.802
0.803

a. BERT(base1): BERT base model with 1 layer neural network
b. BERT(base3): BERT base model with 3 layer neural network
c. BERT(large1): BERT large model with 1 layer neural network
d. BERT(large3): BERT large model with 3 layer neural network
e. BERT(Twitter1): BERT Twitter model with 1 layer neural network
f. BERT(Twitter3): BERT Twitter model with 3 layer neural network

6094
Authorized licensed use limited to: Harvard Library. Downloaded on October 01,2021 at 03:41:55 UTC from IEEE Xplore. Restrictions apply.

TABLE II.
Architecture
BERT(base1)
BERT(base1)
BERT(Twitter3)
BERT(Twitter1)
BERT(Twitter3)

Performance
Measure
Accuracy
Precision
Recall
F1
ROC

RESULT OF PAIRED T-TESTS. FIRGURES IN BOLD ARE > 0.01.
BERT
(base1)

2.84×10-7
9.96×10-6
1.89×10-7

BERT
(base3)
6.28×10-2
1.22×10-2
6.46×10-5
3.54×10-5
5.82×10-6

D. Classification Performance
To be able to compare the differences in classification
performance, ten-fold cross validation were conducted for
each of the 6 configurations/architectures (3 BERT models
times 2 neural networks), and the averages of the
performance measures were collected. For 10-fold cross
validation, our corpus of annotated tweets was split into 10
sets. During each test, one of the 10 sets was used as the
testing set and the other 9 sets were used as the training set.
The same partition of data was applied to each
configuration/architecture, facilitating the statistical analysis
of the performance differences. Paired t-tests were
conducted between each pair of configurations/architectures
studied to test statistically if the performance differences are
due to chance or not.
E. GPU Machine
A self-made GPU machine was used in this project for
learning the BERT model and performing neural networkbased classification. The computer consists of an AMD
RYZEN 7 7200 Eight-core Processor, a 2,304 core NVidia
GeForce RTX 2070 GPU, and 32 GB memory. The 64-bit
Ubuntu (Version 16.04) operating system was installed on
the machine. With this system, it was observed that it took
about 30 minutes to perform 100 steps of training of the
BERT Twitter model.
III. RESULTS
Table I lists the result of classification on 12,331
annotated tweets, with 3 different BERT models and 2
different configurations of neural networks. BERT(baseN)
and BERT(largeN) are pre-trained models released by
Google, where N is either 1 or 3, the number of hidden layer
of neural network. BERT(TwitterN) is the model trained with
3.2 million unlabeled tweets. The figures listed are the
averages of 10-fold cross validation. The bold-faced numbers
are the highest in the respective column (measure). The
measures of precision, recall, and F1 are for the class of
personal experience tweets (PETs).
In Table II, only the configurations with the highest
values of performance measures in Table I are listed on the
left, and paired t-test results between each of them and each
of the 5 other configurations are listed. This is to help
understand whether or not the difference in performance of
each pair of methods is due to chance. The bold-faced figures
are those greater than 0.01.

BERT
(large1)
1.14×10-1
1.12×10-1
1.12×10-5
6.77×10-5
7.33×10-6

BERT
(large3)
4.83×10-2
1.82×10-2
4.28×10-7
3.34×10-5
2.18×10-6

BERT
(Twitter1)
3.97×10-1
1.22×10-2
2.18×10-1

BERT
(Twitter3)
3.97×10-1
6.73×10-2
2.63×10-1

3.61×10-1

IV. DISCUSSIONS
In our testing, both pre-trained BERT models and our
trained
BERT
model
demonstrated
classification
performance in a relatively narrow range (Table I). The pretrained BERT base model with 1 hidden layer neural network
BERT(base1) showed the highest accuracy and precision
values, our trained BERT model showed highest values in
recall and ROC (3 layer hidden neural network –
BERT(Twitter3)), and F1 (1 layer hidden neural network –
BERT(Twitter1)).
Interestingly, the pre-trained BERT large mode,
BERT(largeN), does not have a highest value in any of the
performance measures. This may indicate that this model
may not be best suited for our classification task.
It can be misleading to draw a conclusion of best
performing method based upon the highest values in
performance measures without conducting statistical analysis
(i.e., hypothesis testing). As illustrated in Table II, our paired
t-tests performed on each pair of methods indicated that (1)
the differences in values of accuracy and precision between
BERT(base1) and other methods can be due to chance
because p > 0.01, and (2) the differences in recall, F1, and
ROC between the our trained Twitter model and pre-trained
models do exist with p < 0.01. This suggests that our trained
model performs better than the Google pre-trained BERT
models in recall, F1, and ROC. One possible explanation to
the finding is that tweets we collected are more relevant to
our problem domain than the data sets Google used to train
its BERT models: BooksCorpus and Wikipedia.
The result in Table I can also help guide us on choosing a
most appropriate method for our purpose. For our
classification task, having a higher recall will result in more
true positives from the raw data to be included in the
predicted positive class (PET), and having a higher precision
will have more true positives in the predicted positive class.
In our application, we can choose a method with the highest
product of recall and precision, and BERT(Twitter1) seems
to be our choice, although the statistical analysis of
performance between BERT(Twitter1) and BERT(Twitter3)
does not guarantee that the observed difference is not due to
chance.
This research primarily focused on investigation of
classification performance differences between Google pretrain BERT models and our BERT model trained with
medication-related tweets. One possible future direction can
be the study of classification performance difference between

6095
Authorized licensed use limited to: Harvard Library. Downloaded on October 01,2021 at 03:41:55 UTC from IEEE Xplore. Restrictions apply.

the BERT models and other neural embedding models such
as word2vec [12], GloVe [13], and FastText [14].

[11]

V. CONCLUSION
In this research, Google’s BERT models, which
contextually represent tweet text, along with neural networkbased classifiers were investigated in predicting personal
experience tweets related to medication use. Our BERT
model trained with 3.2 million unlabeled tweets related to
103 medications demonstrated better classification measures
of recall, F1 and ROC than Google’s pre-trained models, and
the statistically analysis confirms the differences in
performance do exist (p < 0.01). This may suggest that
domain-specific data contribute to the better classification
performance, and our model is more appropriate for
predicating personal experience tweets.

[12]
[13]

[14]

embedding and LSTM neural network. BMC bio-informatics, 19(8),
210 (2018).
J. Devlin, M. W. Chang, K. Lee and K. Toutanova. Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018
T. Mikolov, K. Chen, G. Corrado, J. Dean. Efficient Estimation of
Word Representations in Vector Space. In Proceedings of Workshop
at ICLR, 2013.
J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for
word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP), pp.
1532-1543, 2014.
P. Bojanowski, E. Grave, A. Joulin, T. Mikolov. Enriching word
vectors with subword information. arXiv preprint arXiv:1607.04606,
2016.

ACKNOWLEDGMENT
Authors wish to thank anonymous reviewers for their
critiques and constructive comments that helped improve
this manuscript, and to acknowledge these individuals for
their contribution to this project: Dustin Franz, Ravish Gupta
for collecting the Twitter data, Alexandra Vest, Cecelia Lai,
Bridget Swindell, Mary Stroud, and Matrika Gupta for
annotating the tweets.
REFERENCES
[1] G. J, Dal Pan, L. M. Lindquist and G. Kate. "Postmarketing
[2]

[3]

[4]

[5]

[6]

[7]
[8]

[9]

[10]

spontaneous pharmacovigilance reporting systems."
Pharmacoepidemiology 5 (2011): 150-151.
K. D. Haerian, S. Varn, L. Vaidya, H. S. Ena, Chase, and C.
Friedman. "Detection of pharmacovigilance‐related adverse events
using electronic health records and automated methods." Clinical
Pharmacology & Therapeutics 92, no. 2 (2012): 228-234.
D. Hristovski, A. Burgun-Parenthoine, P. Avillach, and T. C.
Rindflesch. "Towards using literature-based discovery to explain drug
adverse effects." In 24th International Conference of the European
Federation for Medical Informatics Quality of Life through Quality of
Information. MIE. 2012.
R. Sloane, O. Osanlou, D. Lewis, D. Bollegala, S. Maskell and M.
Pirmohamed, "Social media and pharmacovigilance: a review of the
opportunities and challenges." British journal of clinical
pharmacology 80, no. 4 (2015): 910-920.
S. Wijeratne, A. Sheth, S. Bhatt, L. Balasuriya, H. S. Al-Olimat, M.
Gaur, A. H. Yazdavar and K. Thirunarayan, “Feature Engineering for
Twitter-based Applications.” Feature En-gineering for Machine
Learning and Data Analytics, 35 (2017).
K. Jiang, R. A. Calix and M. Gupta, Construction of a personal
experience tweet Corpus for health surveillance. Proceedings of the
15th workshop on biomedical natural language processing, pp.128135 (2016).
K. Jiang,Y. Zheng, Mining twitter data for potential drug effects. In
International Con-ference on Advanced Data Mining and
Applications, pp.434-443 (2013). Springer, Berlin, Heidelberg.
N. Alvaro, M. Conway, S. Doan, C. Lofi, J. Overington and N.
Collier, Crowdsourcing Twitter annotations to identify first-hand
experiences of prescription drug use. Journal of biomedical
informatics, 58, 280-287 (2015).
R. A. Calix, R. Gupta, M. Gupta and K. Jiang, Deep gramulator:
Improving precision in the classification of personal health-experience
tweets with deep learning. In 2017 IEEE Inter-national Conference on
Bioinformatics and Biomedicine (BIBM), pp.1154-1159 (2017).
IEEE.
K. Jiang, S. Feng, Q. Song, R. A. Calix, M. Gupta, and G. R, Bernard,
Identifying tweets of personal health experience through word

6096
Authorized licensed use limited to: Harvard Library. Downloaded on October 01,2021 at 03:41:55 UTC from IEEE Xplore. Restrictions apply.

