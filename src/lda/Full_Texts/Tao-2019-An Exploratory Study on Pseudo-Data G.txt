388

MEDINFO 2019: Health and Wellbeing e-Networks for All
L. Ohno-Machado and B. Séroussi (Eds.)
© 2019 International Medical Informatics Association (IMIA) and IOS Press.
This article is published online with Open Access by IOS Press and distributed under the terms
of the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).
doi:10.3233/SHTI190249

An Exploratory Study on Pseudo-Data Generation in Prescription and Adverse Drug
Reaction Extraction
Carson Taoa, Kahyun Leeb, Michele Filanninob, and Özlem Uzunerb
a

The State University of New York at Albany, Albany, NY, USA, bGeorge Mason University, Fairfax, VA, USA

Abstract
Prescription information and adverse drug reactions (ADR)
are two components of detailed medication instructions that
can benefit many aspects of clinical research. Automatic
extraction of this information from free-text narratives via
Information Extraction (IE) can open it up to downstream uses.
IE is commonly tackled by supervised Natural Language
Processing (NLP) systems which rely on annotated training
data. However, training data generation is manual, timeconsuming, and labor-intensive. It is desirable to develop
automatic methods for augmenting manually labeled data. We
propose pseudo-data generation as one such automatic
method. Pseudo-data are synthetic data generated by
combining elements of existing labeled data. We propose and
evaluate two sets of pseudo-data generation methods:
knowledge-driven methods based on gazetteers and datadriven methods based on deep learning. We use the resulting
pseudo-data to improve medication and ADR extraction. Datadriven pseudo-data are suitable for concept categories with
high semantic regularities and short textual spans. Knowledgedriven pseudo-data are effective for concept categories with
longer textual spans, assuming the knowledge base offers good
coverage of these concepts. Combining the knowledge- and
data-driven pseudo-data achieves significant performance
improvement on medication names and ADRs over baselines
limited to the use of available labeled data.
Keywords:
Natural Language Processing, Machine Learning, Information
Storage and Retrieval

Introduction
Electronic health records (EHR) contain significant amounts of
medical information. However, most of this information is
presented in unstructured narrative free-text and remain
unavailable to computerized systems that rely on structured
representations for access and retrieval [1,2]. This unavailable
information includes medication and prescription information,
which is often described in narrative portions of electronic
health records (EHRs) [3] and can complement the information
represented in prescriptions themselves [4].
Natural language processing (NLP) methods such as
information extraction (IE) can open up this information to
downstream uses [2,3], such as comparative effectiveness [5]
cohort selection [6,7], medication reconciliation [8], and
pharmacovigilance and pharmacogenomics [9]. Extraction of
medication information from narratives enhances patient safety
by making this more complete medication information
available to use for clinical decision support [8]. Extraction of
ADRs supports goals highlighted by the U.S. Food and Drug
Administration (FDA) [10] for comparing the ADRs present in

labels from different manufacturers for the same drug, and for
performing post-marketing safety analysis by identifying new
ADRs not currently present in the labels [11].
Over the last few decades, there have been many efforts for
extracting information from clinical narratives [4,11]. An
important factor in the development of these systems is the
availability of sufficient training data [12]. However,
preparation of training data is time-consuming and laborintensive; as a result, such data sets are scarce. Even when such
data are available, some concept types in the data tend to be
sparsely-represented [13]. NLP systems either relatively
underperform on concept categories that are more sparse or
they complement their supervised machine-learning models
with hand-build rules [14,15]. An alternative to these
approaches requires augmenting labeled data with pseudo-data.
Pseudo-data are synthetic data that are derived from samples in
labeled data, e.g., by combining elements of existing samples.
Despite its promise for IE, to the best of our knowledge,
pseudo-data generation has received limited attention in the
clinical domain. Instead, researchers have focused on
modifying the architectures of their systems to improve
recognition of concepts, potentially over-fitting their systems to
the data and to the task [4]. We aim to address this gap in the
literature.
Our proposed approach develops and compares knowledgeand data-driven pseudo-data generation methods for IE from
narratives of EHRs [4] and from drug labels [11]. We use the
resulting pseudo-data to enhance NLP methods for extracting
medication and ADR information. Despite focus on medication
names and ADRs, we also study concepts that relate to
medications and ADRs. Our IE methods extract concepts in a
sentence; therefore, improving identification of a key concept
in a sentence can have a spillover effect and help extraction of
the rest of the concepts in the sentence, even when those
concepts themselves are not directly enhanced with pseudodata. We, therefore, report results on the following set of
concept types: medication names, dosages, frequencies, modes,
durations, and reasons; ADRs, animal species, drug classes,
factors, negated phrases, and severities. Our results show that
pseudo-data helps improve performance on medication names
and ADRs over methods that are limited to the use of available
labeled data.

Related Work
Extraction of ADR and medication information is commonly
solved using rules and/or machine learning classifiers. Rulebased systems have proven to be useful [4,14] for this purpose
but come with limitations on performance, scalability, and
generalizability [16]. They suffer particularly on concept types
that show much syntactic variation [13]. In response to these
limitations of rule-based systems, most high-performing

C. Tao et al. / An Exploratory Study on Pseudo-Data Generation in Prescription and Adverse Drug Reaction Extraction

methods utilize machine learning classifiers [13,15-17]. The
primary challenge facing these systems is data sparsity [13]. As
a result, machine learning solutions often require support from
rules [16] and have given promising results. The current state
of the art in medication extraction is one such hybrid system
that combines a machine learning classifier with postprocessing rules and achieves 89.6% F-measure on medication
names in narrative EHRs [15]. In comparison, the current state
of the art in ADR extraction is a machine learning system that
achieves 82% F-measure [17] on ADRs. Given their reliance on
labeled data, performance on both tasks can be improved by
more data. Given the high cost of generating manual gold
standard data, we explore an automatic method, pseudo-data
generation, for augmenting available labeled data. To assess
generalizability, we incorporate pseudo-data into two tasks
tackled on two different kinds of corpora: medication extraction
from EHRs and ADR extraction from drug labels.
Pseudo-data generation is a common way of resolving
imbalanced datasets. Despite its potential benefit for addressing
data sparsity, pseudo-data generation remains relatively
unexplored in the clinical domain. Outside of the clinical
domain, Chawla et al. [18] proposed an oversampling method,
called synthetic minority over-sampling technique (SMOTE),
that generates pseudo-samples for minority classes by joining
existing samples with their nearest neighbors in terms of feature
vectors. SMOTE takes the size of the largest class as a
parameter to produce a fully balanced training set for all
classes. Within the clinical domain, pseudo-data generation has
been tackled only in very few studies: e.g., Keretna et al. [19]
formulated an extended segment representation technique to
improve biomedical IE, with emphasis on ambiguous named
entities.
In our study, we evaluate the efficacy of pseudo-data in clinical
IE on medications and ADRs. We propose two methods for
pseudo-data generation. The first method utilizes a combination
of random duplication and external gazetteers. Being one of the
simplest oversampling methods, random duplication on its own
is rarely effective [18]. Literature [20] fails to show significant
improvement on minority class recognition when data sizes of
the minority class are increased through repetition of existing
data. We expect that this finding results from lack of any “new”
information. Therefore, we incorporate information from
external gazetteers and generate new samples from them so that
we can address the sparsity of unique samples. The second
method takes advantage of word embeddings. Pennington et al.
[21] have shown that semantically similar words cluster
together in the word embedding space. Given this, we generate
pseudo-data by replacing tokens (from clinically-relevant
concepts) in the existing training data with their nearest
neighbors in terms of embeddings. After generating pseudodata, we use them to enrich our training data. We hypothesize
that pseudo-data can help improve IE performance.

Data
We base our studies on two established NLP data sets for
medication and ADR extraction. Our first corpus consists of
991 discharge summaries from Partners Healthcare and was the
basis for the 2009 i2b2 Medication Challenge [4] which
focused on the extraction of medication and prescription
information from narratives of EHRs. This dataset contains 145
annotated summaries and 846 unannotated summaries in the
training set. The test set contains 252 annotated summaries. Our
second data set consists of 2,309 drug labels from the FDA and
were the basis for the 2017 TAC ADR Challenge [11], which
focused on the extraction of ADRs, related drug classes, ADR

389

severities, and other information from narratives. This dataset
contains 101 annotated labels and 2,208 unannotated labels in
the training set. The test set contains 99 annotated labels. In our
study, we build systems based on the training data. We then run
an end-to-end system against the test set. See Table 1 for the
per-category statistics for each dataset.
Table 1 - Number of records, tokens, and phrase-level
concepts per category (training and test set)

Methods
We study pseudo-data generation on two types of concepts,
medication names and ADRs, which lie at the heart of any
prescription and ADR extraction task. Given our focus on
pseudo-data generation, we first introduce a baseline IE system
that can extract these concepts from labeled data. We then
present two pseudo-data generation methods and measure the
contribution of pseudo-data to the gold standard data and to
extraction performance.
Creating a Baseline IE System
Our baseline IE system [16] uses conditional random fields
(CRFs) [22] trained with: (1) Normalized tokens: lower-cased
tokens with punctuation or special characters removed. We
replace all numbers with a generic placeholder (e.g., 300 ml →
DDD ml, 3rd degree → D degree). (2) Part-of-speech (POS)
tags. (3) Temporal properties: A set of seven binary features
that indicate whether a token represents a signal for temporal
expressions such as time, temporal period, and temporal
reference. (4) Real-valued word embeddings trained on the
unannotated portions of our datasets, as indicated by prior work
[23]. When including these features into the training process,
we use window size of ±2 around the target word. This window
size is selected as the optimal parameter from cross-validation
experiments which checked window sizes from ± 1 to ± 5 in our
previous study [16].
We evaluated this system on both ADR and medication
extraction using only gold standard labeled data (see Table 2
for results [16]). We then enhanced it with pseudo-data as
follows: we collected sentences from the annotated training
data that contained at least one ADR or medication name. We
generated pseudo-data based on these sentences using two
different methods: Knowledge-driven and data-driven.
Generating Knowledge-Driven Pseudo-data
This method takes advantage of external gazetteers. Using
ADR extraction as an example for describing the methodology,
we select and access a database (i.e., Vigibase from
VigiAccess.org) that can serve as a lookup table for ADRs [23].
In our study, most ADR concepts found in drug labels use
normalized terms and controlled vocabularies (i.e., the FDA

390

C. Tao et al. / An Exploratory Study on Pseudo-Data Generation in Prescription and Adverse Drug Reaction Extraction

publishes regulations governing the content and format of ADR
information [10]). With the help of Vigibase, we manually
create a gazetteer that contains 18,310 ADRs commonly found
in the drug labels. We then randomly select one sentence from
the ones previously collected from our gold standard and
replace the ADR in that sentence with a different one from the
gazetteer. If we have more instances in the gazetteer than the
total number of sentences that contains ADRs in the gold
standard, then we continue this step until all instances in the
gazetteer are injected to the sentences in the gold standard,
creating at least one pseudo-data sentence per original that
contains an ADR. For example, the original sentence “the most
common adverse reactions (>=20%), regardless of causality,
were stomatitis, infections, and asthenia” becomes “the most
common adverse reactions (>=20%), regardless of causality,
were dizziness, pyrexia, and fatigue”. We use the same method
for pseudo-data generation for medication names, creating a
gazetteer from Drugs.com which contains 5,394 commonly
used medication names.
Generating Data-Driven Pseudo-data
This method takes advantage of patterns observed in the data.
In particular, we study similarity of concepts using word
embeddings. We utilize word embeddings trained on MIMICIII (a large database of about 2 million clinical notes for about
46 thousand patients [24]). For each sentence containing an
ADR/medication, we replace each token within the concept
with the top three closest tokens represented by word
embeddings (i.e., replacing each ADR/medication token with
its first, second, and third nearest neighbor, respectively) to
create pseudo-concepts. For multi-token concepts, we create
multiple pseudo-concepts by combinatorically putting together
the nearest neighbors of each of their constituent tokens. We
measure word-to-word distances using Euclidean distance, as
suggested by the literature [21] as a proxy for semantic
similarity. This measure of similarity between tokens is a
likelihood rather than a certainty; however, it has been widelyaccepted and used in NLP with reliable results [21]. The
purpose of the data-driven pseudo-data is to enrich the training
data with more tokens that resemble ADRs/medication names
[25-27]. We do not aim to replace each word with its synonym.
Rather, we aim to replace each concept with a concept that is
semantically similar or related [25-27], and is of the same type,
i.e., replace a drug with a drug in order to improve learning
contexts in which drugs occur.
Unlike knowledge-driven pseudo-data that is guaranteed to
contain pseudo-concepts of the correct type, concepts that are
related in the word embedding space may not all be concepts of
the same type. In order to identify and utilize the most useful
pseudo-concepts from data-driven pseudo-data, we use
confidence thresholds and filter examples with lower
confidence. For this, we start with training a supervised model
using labeled data. We apply this model to predict concepts on
the data-driven pseudo-data. We obtain confidence rates for
each of the predicted labels and utilize sentences from pseudodata that contain at least one ADR concept (or medication
concept) that scores higher than the confidence threshold. We
repeat this process until no ADRs/medications in the pseudodata are introduced to the labeled data.
The literature suggests picking a confidence threshold of 0.90
or 0.95 [28] for filtering. In these cases, the system only utilizes
high-confidence concepts from the pseudo-data. Such high
thresholds would minimize noise that can come from incorrect
pseudo-concepts. However, setting a high confidence threshold
also potentially excludes some correctly predicted pseudoconcepts from the training model. Given sufficient samples of

training data, the optimal threshold can be tuned via crossvalidation. In our case, we observe that a threshold of 0.85
would filter out some potentially useful pseudo-concepts.
Therefore, we set the threshold to 0.8 in order to be conservative
without being overzealous. Figure 1 shows some example
pseudo-concepts.

Figure 1 - Examples of data-driven pseudo-concepts
Generating Hybrid Pseudo-Data
Knowledge- and data-driven methods can generate different
pseudo-data. Therefore, as a final step, we combine these two
datasets to create a hybrid pseudo-dataset. Figure 2 shows the
workflow we designed to generate the hybrid pseudo-data.

Figure 2 - Generating hybrid pseudo-data

Results and Discussion
We tune the parameters of our models with five-fold crossvalidation on the training set. Then we train our IE system on
the complete training set and evaluate on the test set for each
task. Feature sets and window sizes remain identical to those
described in Baseline IE System section.
Table 2 shows phrase-level F1-measure with and without
pseudo-data on the test set for both IE tasks. When extracting
ADRs and medication names, with respect to the baseline using
no pseudo-data, the model performs significantly better when
enhanced with either knowledge-driven or data-driven pseudodata. The performance gain also spills over to other concept
categories, with significant performance improvement on the
severity class.
In medication name extraction, the model with data-driven
pseudo-data performs significantly better than the model with
knowledge-driven pseudo-data. However, there is no
significant advantage of data-driven pseudo-data in ADR
extraction when compared to knowledge-driven pseudo-data.
Manual analysis of the system output and pseudo-data showed
that medication names are mostly single token nouns in the
MIMIC III vector set. As a result, these concepts can be used
interchangeably without breaking the surrounding context and
can benefit IE performance. In contrast, in spite of 53.60% of
the ADRs being single-token concepts in the test set, many of

C. Tao et al. / An Exploratory Study on Pseudo-Data Generation in Prescription and Adverse Drug Reaction Extraction

them have longer textual spans. Given that our data-driven
methods utilize word embeddings at the token level, even after
filtering through confidence thresholds, these pseudo-concepts
may not benefit IE.
Our results and manual analyses suggest that data-driven
pseudo-data are suitable for concept categories with short
textual spans, whereas the knowledge-driven pseudo-data are
effective for concept categories with longer textual spans,
assuming that the gazetteer offers a good coverage of the
concept types in focus. Hybrid pseudo-data puts the strengths
of the two together and shows statistically-significant
performance improvement on the extraction of both ADRs and
medication names. The biggest gains come from improved
performance on concepts that appear only once in the data. For
example, on the ADR extraction task, 10.92% of ADR concepts
fall under this category. Furthermore, 53.95% of these ADRs
appear in the VigiAccess gazetteer. We also checked the ADRs
that were not extracted by the model with knowledge-driven
pseudo-data but by the model with hybrid pseudo-data. The
model with hybrid pseudo-data performed particularly well on
ADRs that are modified by adjectives or prepositional phrases.
For instance, unlike the model with hybrid pseudo-data, the
knowledge-driven model failed to recognize “peripheral
sensory neuropathy” and “residual neuropathy” when
“sensory” and “residual” are modifying the ADR term
(“peripheral neuropathy”).
Table 2 - Phrase-level F1-measure with and without pseudodata on the test set for both IE tasks

391

“No Doz” and “One A Day” is problematic because the first
one starts with a negated token, whereas the second one looks
like a medication frequency. In ADR extraction, generic ADRs
are also a common theme in erroneous predictions (e.g., “gi
adverse reactions”, “autoimmune disorders”, “interstitial lung
disease”). Furthermore, some ADR concepts contain (or are
surrounded by) multiple generic concepts and are syntactically
complex, (e.g., “irreversible vision damage with vision
damage from SABRIL”, “structure infections with
moderate/severe pre-existing renal impairment”, “drug
reaction with eosinophilia and systemic symptoms”). What
is more, some concepts contain negated tokens and mislead the
system. For example, the model failed to extract “negative
thoughts” and “non-melanoma skin cancer” due to the
included negation signals, “negative” and “non”.
Other errors arise because of misleading lexical context. For
example, “alcoholic beverage” and “vomiting” are both
predicted as medication names when appearing respectively in
“she took 600 ml alcoholic beverage for the day” and “vomiting
x 5 hours”. Similarly, the system does not recognize animal
concept “rodent” in “rodent models inhibition of IL-12/IL23p40” because it is presented with a protein product. Finally,
our system still misses concepts that have longer textual spans
or lack temporal signals (e.g., “disruption of the body ability
to reduce core body temperature”, “progression of pre
chronic myelomonocytic leukemia with nras mutation”,
“after your cardiac catheterization”, “possibly lifelong”).
In addition, our method faces its biggest challenges when
differentiating certain concepts that look like ADRs but are not
caused by a reaction to a drug (and are therefore not ADRs).
These include: (1) Concepts that look like ADRs but actually
refer to the medical problems of patients. For example, in “fever
and persistent constipation upon admission”, where the
underlined concepts are not ADRs. This is the most prominent
type of challenge. (2) Hypothetical or negated ADRs that do not
actually occur. For example, “evaluate if neuropathy is
suspected” and “if muscle signs and symptoms persist”.
Our future work will focus on these challenges and explore
more comprehensive knowledge-bases, such as the Unified
Medical Language System (UMLS) [29] as well as phrase-level
[30] or sentence-level embeddings [31] that can generate more
reliable pseudo-data for the training models.

The model with hybrid pseudo-data also shows statistically
significant gains on severity. However, it fails to show any
performance gains against the baseline (differences are not
statistically significant) on the rest of the minority classes.
These concept categories have limited training samples
therefore the classifier faces difficulties when generalizing
patterns. For example, there are only 44 samples (5 unique
samples) for Animal class in the training data. Therefore, the
model fails to recognize other animal entities in the test data
(e.g., “cynomolgus”, “minipig”, “ferret”).
Error Analysis and Limitation
We analyzed the system’s output to pinpoint incorrect
predictions with respect to the gold standard. In medication
extraction, we found the model still cannot distinguish some
generic medication names (e.g., “his medication”, “other
antibiotics”, “this medication”, “trial drug”). We found 70
instances related to “medication(s)”, 53 instances related to
“antibiotic(s)”, and 14 instances related to “home” in the
remaining errors. Also, the system cannot recognize some
ambiguous medication names. For example, the extraction of

Conclusion
In this paper, we evaluated the contribution of pseudo-data to
clinical IE, using ADR and medication extraction as test bed
applications. We developed two methods for generating
pseudo-data. The first method is knowledge-driven and uses
gazetteers that contain clinically-relevant concepts, whereas the
second method is data-driven and takes advantage of word
embeddings. We found that the data-driven method is suitable
for concept categories with short textual spans, whereas the
knowledge-driven methods are effective for concept categories
with longer textual spans, assuming the gazetteers offer good
coverage of the concepts within each category. Hybrid pseudodata takes advantage of strengths of both, improving
performance on the extraction of two important concept
categories in clinical IE: ADRs and medication names.

Acknowledgements
This work was supported by Philips Research North America,
PI: Peter Szolovits. The content is solely the responsibility of

392

C. Tao et al. / An Exploratory Study on Pseudo-Data Generation in Prescription and Adverse Drug Reaction Extraction

the authors and does not necessarily represent the official
views of the sponsors.

References
[1] Mønsted T, Reddy MC, Bansler JP. The use of narratives
in medical work: A field study of physician-patient consultations. In ECSCW 2011: Proceedings of the 12th European Conference on Computer Supported Cooperative
Work, 24-28 September 2011, Aarhus Denmark 2011 (pp.
81-100). Springer, London.
[2] Uzuner Ö, Stubbs A. Practical applications for natural language processing in clinical research: The 2014
i2b2/UTHealth shared tasks. Journal of biomedical informatics. 2015 Dec;58(Suppl):S1.
[3] Savova GK, Masanz JJ, Ogren PV, Zheng J, Sohn S, Kipper-Schuler KC, Chute CG. Mayo clinical Text Analysis
and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the
American Medical Informatics Association. 2010 Sep
1;17(5):507-13.
[4] Uzuner Ö, Solti I, Cadag E. Extracting medication information from clinical text. Journal of the American Medical
Informatics Association. 2010 Sep 1;17(5):514-8.
[5] Capurro D, van Eaton E, Black R, Tarczy-Hornoch P.
Availability of structured and unstructured clinical data for
comparative effectiveness research and quality improvement: a multisite assessment. EGEMS. 2014;2(1).
[6] Klassen P, Xia F, Vanderwende L, Yetisgen M. Annotating clinical events in text snippets for phenotype detection.
In Proceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC-2014) 2014.
[7] Mehrabi S, Krishnan A, Roch AM, Schmidt H, Li D, Kesterson J, Beesley C, Dexter P, Schmidt M, Palakal M, Liu
H. Identification of patients with family history of pancreatic cancer-Investigation of an NLP System Portability.
Studies in health technology and informatics.
2015;216:604.
[8] Cornish PL, Knowles SR, Marchesano R, Tam V, Shadowitz S, Juurlink DN, Etchells EE. Unintended medication
discrepancies at the time of hospital admission. Archives
of internal medicine. 2005 Feb 28;165(4):424-9.
[9] Wilke RA, Xu H, Denny JC, Roden DM, Krauss RM,
McCarty CA, Davis RL, Skaar T, Lamba J, Savova G. The
emerging role of electronic medical records in pharmacogenomics. Clinical Pharmacology & Therapeutics.
2011 Mar;89(3):379-86.
[10] Drug OD. Guidance for Industry. Center for Drug Evaluation and Research (CDER). 2011 Oct;1000.
[11] Roberts K, Demner-Fushman D, Tonning JM. Overview
of the TAC 2017 Adverse Reaction Extraction from Drug
Labels Track. In Proceedings of the 2017 Text Analysis
Conference. 2017 Nov 13-14. National Institute of Standards and Technology.
[12] Friedman C, Johnson SB. Natural language and text processing in biomedicine. In Biomedical Informatics 2006
(pp. 312-343). Springer, New York, NY.
[13] Tao C. Clinical information extraction from unstructured
free-texts. State University of New York at Albany. 2018.
[14] Xu H, Stenner SP, Doan S, Johnson KB, Waitman LR,
Denny JC. MedEx: a medication information extraction
system for clinical narratives. Journal of the American
Medical Informatics Association. 2010 Jan 1;17(1):19-24.
[15] Patrick J, Li M. A cascade approach to extracting medication events. In Proceedings of the Australasian Language
Technology Association Workshop 2009 (pp. 99-103).

[16] Tao C, Filannino M, Uzuner Ö. Prescription extraction
using CRFs and word embeddings. Journal of biomedical
informatics. 2017 Aug 1;72:60-6.
[17] Xu J, Lee HJ, Ji Z, Wang J, Wei Q, Xu H. UTH_CCB
System for Adverse Drug Reaction Extraction from Drug
Labels at TAC-ADR 2017. In Proceedings of the 2017
Text Analysis Conference. 2017 Nov 13-14. National Institute of Standards and Technology.
[18] Chawla NV, Bowyer KW, Hall LO, et al. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research. 2002 Jun 1;16:321-57.
[19] Keretna S, Lim CP, Creighton D, et al. Enhancing medical named entity recognition with an extended segment
representation technique. Computer methods and programs
in biomedicine. 2015 Apr 1;119(2):88-100.
[20] Japkowicz N. The class imbalance problem: Significance
and strategies. In Proceedings of the International Conference on Artificial Intelligence 2000 Jun 26.
[21] Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language processing (EMNLP) 2014 (pp. 1532-1543).
[22] Okazaki N. Crfsuite: a fast implementation of conditional random fields (CRFs). 2017.
[23] Tao C, Lee K, Filannino M, Buchan K, Uzuner Ö. Extracting and Normalizing ADRs from Drug Labels. In Proceedings of the 2017 Text Analysis Conference. 2017 Nov
13-14. National Institute of Standards and Technology.
[24] Johnson AE, Pollard TJ, Shen L, Li-wei HL, Feng M,
Ghassemi M, Moody B, Szolovits P, Celi LA, Mark RG.
MIMIC-III, a freely accessible critical care database. Scientific data. 2016 May 24;3:160035.
[25] Henry S, Cuffy C, McInnes BT. Vector representations of
multi-word terms for semantic relatedness. Journal of biomedical informatics. 2018 Jan 1;77:111-9.
[26] McInnes BT, Pedersen T. Evaluating semantic similarity
and relatedness over the semantic grouping of clinical term
pairs. Journal of biomedical informatics. 2015 Apr
1;54:329-36.
[27] Henry S, McQuilkin A, McInnes BT. Association
measures for estimating semantic similarity and relatedness between biomedical concepts. Artificial intelligence
in medicine. 2018 Sep 7.
[28] Abney, S. Semi-supervised learning for computational
linguistics. 2007. CRC Press.
[29] Bodenreider O. The unified medical language system
(UMLS): integrating biomedical terminology. Nucleic acids research. 2004 Jan 1;32(suppl_1):D267-70.
[30] Yu M, Dredze M. Learning composition models for
phrase embeddings. Transactions of the Association for
Computational Linguistics. 2015 Dec;3:227-42.
[31] Conneau A, Kruszewski G, Lample G, Barrault L, Baroni
M. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. arXiv preprint
arXiv:1805.01070. 2018 May 3.
Addresses for Correspondence
Carson Tao, mtao@albany.edu
UAB 431, 1400 Washington Avenue, Albany, NY, USA, 12222
Özlem Uzuner, ouzuner@gmu.edu
Nguyen Engineering Building, Rm. 5359, 4400 University Drive,
Fairfax, VA, USA, 22030

