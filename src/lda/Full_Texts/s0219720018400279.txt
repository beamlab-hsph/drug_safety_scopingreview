Journal of Bioinformatics and Computational Biology
Vol. 16, No. 6 (2018) 1840027 (22 pages)
#
.c The Author(s)
DOI: 10.1142/S0219720018400279

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

Wen Juan Hou* and Bamfa Ceesay†
Department of Computer Science and Information Engineering
National Taiwan Normal University
No 88,Tingzhou Road, Sec. 4
Taipei 116, Taiwan R.O.C
*emilyhou@csie.ntnu.edu.tw
†
bmfceesay@csie.ntnu.edu.tw
Received 26 September 2018
Accepted 29 October 2018
Published 19 December 2018
Information on changes in a drug's e®ect when taken in combination with a second drug, known
as drug–drug interaction (DDI), is relevant in the pharmaceutical industry. DDIs can delay,
decrease, or enhance absorption of either drug and thus decrease or increase their action or cause
adverse e®ects. Information Extraction (IE) can be of great bene¯t in allowing identi¯cation
and extraction of relevant information on DDIs. We here propose an approach for the extraction
of DDI from text using neural word embedding to train a machine learning system.
Results show that our system is competitive against other systems for the task of extracting
DDIs, and that signi¯cant improvements can be achieved by learning from word features and
using a deep-learning approach. Our study demonstrates that machine learning techniques such
as neural networks and deep learning methods can e±ciently aid in IE from text. Our proposed
approach is well suited to play a signi¯cant role in future research.
Keywords: Drug–drug interaction; word embedding; neural networks; data abstraction; long
short term memory (LSTM).

1. Introduction
Recent research demonstrates an increasing interest in applying machine learning
and natural language processing to drug–drug interactions (DDIs). DDI refers to a
change in the e®ect of one drug in the presence of another drug.6 DDIs occur during
the co-administration of medications. An added drug may increase or decrease the
e®ect of the initial drug, or it may lead to an adverse e®ect that is not normally
associated with either drug. As such adverse drug reactions (ADRs) may lead to an
* Corresponding

author.
This is an Open Access article published by World Scienti¯c Publishing Company. It is distributed under
the terms of the Creative Commons Attribution 4.0 (CC-BY) License. Further distribution of this work is
permitted, provided the original work is properly cited.
1840027-1

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

increase in drug-safety incidents and healthcare costs, their prevention is of interest.12 There is great potential bene¯t in extracting DDI information from biomedical
texts, using information extraction (IE) techniques. Databases currently listing
known DDIs include Dailymed,11 DrugBank34 and Medscape.21
However, these databases cover only a fraction of DDIs, while many more DDIs
are hidden in the biomedical literature, which constitutes a tremendously rich and
continuously growing source of information. Currently, the National Library of
Medicine's Medline database contains about 23.5 million bibliographic citations and
abstracts from more than 5600 biomedical journals, and about 100,000 new papers
are added every year.25 It is apparent that searching for DDIs in this overwhelming
amount of bioscience literature can be managed e±ciently only with the help of
automated text mining techniques. Accordingly, novel methods to automatically
extract biomedical relations from the literature have attracted signi¯cant interest.
Since a vast amount of biomedical textual information is continuously being produced at an increasing rate, a large number of up-to-date DDIs are hidden in journal
papers, technical reports, and adverse event reporting systems. There is a crucial
need to automatically extract newly discovered DDIs from scienti¯c publications.29
DDIExtraction challenges were held in 201128 and 201329 with the aim of promoting the implementation and comparative assessment of natural language processing techniques in the pharmacovigilance domain. In the 2013 Shared Task, DDIs
had to be classi¯ed into four prede¯ned types: advice, e®ect, mechanism and
int.29Advice is assigned when a recommendation regarding the concomitant use of
two drugs involved is described-for example, the sentence: \Concurrent therapy with
ORENCIA and TNF antagonists is not recommended." E®ect is assigned when
the e®ect of the DDI is described. This can be a pharmacological e®ect, a clinical
¯nding, sign or symptom, an unspeci¯c modi¯cation of the e®ect or action of one of
the drugs, an increase in toxicity, a protective e®ect, or therapeutic failure. An
example is the sentence: \This may indicate that ibuprofen could enhance the
toxicity of methotrexate." Mechanism is assigned when a pharmacokinetic mechanism (a process by which a drug is absorbed, distributed, metabolized or excreted) is
described as a®ected. An example is the sentence: \Concomitant use of calcium
supplements and L-lysine may increase calcium absorption." Int is assigned when
the sentence simply states that an interaction occurs but does not provide any
information about the interaction. For example, the sentence: \A possible drug interaction of FOSCAVIR and intravenous pentamidine has been described." is type
int. Finally, an example of an absence of interaction between two drugs is:
\Imipramine and clonazepam did not change fasting or overload glycemia."
In addition, several similarity-based mining techniques have been applied to
solve this problem. Abdelaziz et al.3 designed a large-scale similarity-based framework (Tiresias) to predict DDIs through link prediction. Tiresias was ¯rst used to
perform a semantic integration of various drug-related sources, and to produce a
knowledge graph describing drug attributes and relations with various related entities such as enzymes, chemical structures, and pathways. The knowledge graph
1840027-2

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

was then used to compute several similarity measures between all included drugs.
The results showed a maximal F-score of 0.85. Cao et al.8 developed a multiple
evidence fusion (MEF) method for the large-scale prediction of adverse drug reactions, based on reference similarity determined by collaborative ¯ltering. This
method integrated multiple similarity measures from various data types, such as
node attribute-based and network structure-based. Sridhar et al.31 proposed a
probabilistic approach for jointly inferring unknown DDIs from a network of multiple drug-based similarities and known interactions. They achieved a maximal
F-score of 0.7. Vilar et al.33 described a similarity-based model which integrated a
reference standard database of known DDIs with drug similarity information
extracted from various sources, such as 2D and 3D molecular structure, interaction
pro¯le, target and side-e®ect similarities.
Another approach to uncovering DDIs is to employ the syntactic information in
texts. Zheng et al.36 presented a graph kernel which made full use of various contexts
to identify DDIs from biomedical literature. In their approach, the relationships
among long-range words as well as close-range words were obtained by graphical
representation of a parsed sentence. Context vectors of the vertex were built to
capture the direct and indirect information of substructures. The kernel integrating
the distance between context vectors was used to detect DDIs. When this approach
was applied to the DDIExtraction 2013 corpus, the F-score of the results was 0.68.
Rule-based techniques have also proven a popular method for extraction of biomedical relations. Usually the rules are either identi¯ed manually or automatically
learned using features from the context in which the relations occur. If there are a
large number of name variants and ambiguous terms in the context, it may cause an
accumulation of rules.10 Rule-based approaches bene¯t from a higher level of precision, but often su®er from signi¯cantly lower recall.26 On the one hand, rules may
be manually constructed. To develop high-precision information retrieval tools,
Abacha and Zweigenbaum2 manually designed linguistic patterns based on selected
sentences from PubMed Central papers to identify treatment relations. Lee et al.19
used the UMLS (Uni¯ed Medical Language System) semantic network20 to infer
relations between medical concepts, and then targeted the precise extraction of
treatment relations between drugs and diseases. In this way, manually written linguistic patterns were constructed from Medline abstracts in the domain of colon
cancer treatment. On the other hand, automated rule mining techniques may be
employed. Embarek and Ferret13 presented a method for text mining in the medical
domain using linguistic patterns. The patterns were learnt automatically from a
manually annotated corpus using an edit distance algorithm. Gopalakrishnan et al.15
developed a Bayesian rule-learning system to generate a set of probabilistic rules.
Hou and Chen16 proposed a rule-learning approach based on syntactic information,
in which a set of rules is automatically generated. Jung and Lee17 used the
FP-growth algorithm,7 an association rule mining algorithm, to generate a clinical
attribute combination pro¯le of each disease. Their results have potential for suggesting disease pairs as new candidates for drug repositioning. Kim et al.18
1840027-3

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

introduced an IE system that requires only sentences labeled as relevant or not to a
given topic by domain experts. Their results showed that the IE system could annotate proteins with a set of extracted relations by learning relations and IE rules for
diseases, functions and structures from only such relevance information.
The above studies demonstrate that extraction of DDI is of interest to many
researchers. In the present study, we propose a method for ¯nding and classifying
DDIs hidden in the biomedical literature by employing a neural embedding method
to train a machine learning system. The advantages of machine learning approaches
include automatical identi¯cation of implicit relationships in datasets, enabling objective quanti¯cation of knowledge, and handling of both large and small information
amounts.
2. Experimental Data
The DDIExtraction 2013 Shared Task29 provided a benchmark DDI corpus which
was used in this paper. This is an annotated corpus that contains gold standard data
for training and test purposes. Figure 1 shows an example of an annotated document
of the DDI corpus.
As shown in Fig. 1, \document id" is the identi¯er of the document; \sentence id"
is the identi¯er of the sentence where text is the context of the sentence; \entity id" is
the identi¯er of the drug in the sentence where \text" identi¯es the drug name,
\type" is the type of the drug, and \charO®set" is the position of the drug in the
sentence; \pair id" is the identi¯er of the drug pair where \ddi" shows whether the
drug pair results in a DDI, and \e1" and \e2" are drug identi¯ers. A total of 5,021
DDIs annotated from 730 DrugBank texts and 175 Medline abstracts were contained
for classi¯cation. As noted in the Introduction section, DDIs were classi¯ed into four
prede¯ned DDI types: advice, e®ect, mechanism and int. Examples and descriptions

−

−
−

−

− −

−

−

−
−

−

−

−

−

Fig. 1. An example of an annotated document of the DDI corpus.
1840027-4

Extraction of drug–drug interaction using neural embedding
Table 1. Examples and description for the four classi¯cation types in DDI Extraction 2013 Shared Task.
Type
Advice (ADV)

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

E®ect (EFF)

Mechanism (MEC)

Int (INT)

False

Description

Example sentence

The sentence is a recommendation or
advice regarding the concomitant
use of two drugs involved.
The sentence describes the e®ect of the
DDI. This can be a pharmacological e®ect, a clinical ¯nding, signs or
symptoms, an unspeci¯ed modi¯cation of the e®ect or action of one
of the drugs, an increase of the
toxicity or a protective e®ect, or
therapeutic failure.
The sentence describes a process by
which drugs are absorbed, distributed, metabolized or excreted.
The sentence merely states that an
interaction occurs and does not
provide any information about the
interaction.
No interaction of two drugs is indicated in the sentence.

Concurrent therapy with ORENCIA
and TNF antagonists is not
recommended.
This may indicate that ibuprofen
could enhance the toxicity of
methotrexate.

Concomitant use of calcium supplements and L-lysine may increase
calcium absorption.
A possible drug interaction of
FOSCAVIR and intravenous
pentamidine has been described.
Imipramine and clonazepam did
not change fasting or overload
glycemia.

for these four types as well as type false are listed in Table 1. The bolded word
identi¯es a drug.
The DDI corpus was split to build separate datasets for training and evaluation.
We randomly selected 572 DrugBank texts and 142 Medline abstracts for the
training dataset, and the remaining 158 DrugBank texts and 33 Medline abstracts
were used for the test dataset. A detailed description of the method used to collect
and process documents can be found in the ¯rst DDIExtraction Shared Task.28
Statistical data on the corpus are listed in Table 2. Because the corpus is imbalanced,
over-sampling was applied.
Table 2. Statistical data on the DDI corpus.
Data type
Training data
Total documents
Total sentences
Pair sentences
Total pair
True pair sentences
Advice/E®ect/Int/Mechanism
False pair sentences
Test data
Total documents
Total sentences

Medline

DrugBank

Medline and DrugBank

142
1301
533
1787
232
8/152/10/62
1555

572
5675
3256
26,005
3789
819/1548/178/1260
22,216

714
6976
3789
27,792
4021
827/1700/188/1322
23,771

33
326

158
973

191
1299

1840027-5

W. J. Hou & B. Ceesay

3. System Architecture

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

The system architecture proposed in this study is intended to identify DDIs from
biomedical literature. Our system is composed of two overall processing stages.
(1) Learning and identi¯cation of DDI entity pairs: This ¯rst stage is intended
to detect whether any DDIs exist within the test data. It involves binary labeling
of entity pairs in a given expression as interacting or noninteracting.
(2) Learning and identi¯cation of DDI types: This second stage is intended to
determine the classi¯cation of each DDI into one of the four types: advice, e®ect,
mechanism and int.
Figures 2 and 3 present the work°ows for DDI detection and classi¯cation of DDI
pairs into types. The DDI detection model consists of three important processing
stages, (1) preprocessing, (2) neural embedding, and (3) model classi¯er (Fig. 2).
Figure 3 depicts the DDI type classi¯cation model used in this study, and follows the
same processing stages as shown in Fig. 2. In both cases, the model classi¯er is a Long
Short Term Memory (LSTM) neural network with three hidden layers. In the DDI
pair detection scheme, there are two nodes in the output layer, one representing true
and the other false. Hence, the classi¯er model for DDI pair detection is a binary
classi¯er. In contrast, the DDI type classi¯er model has ¯ve nodes in the output layer,
each corresponding to one of the ¯ve DDI pair types listed in Table 1. In both models,
softmax24 was used to enable multiple nodes at the output layer, and the number of
nodes or neurons in hidden layers was set as the mean of the nodes or neurons in the
input and output layers. The number of nodes in the input layer was determined by

Fig. 2. DDI pair detection work°ow.
1840027-6

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

Fig. 3. Classi¯cation task for DDI Types.

the size of the vectors from the neural embedding, which correspond to the number of
features (columns) in the data.
Stage 1 accomplishes what is called a recognition task, and stage 2 accomplishes a
classi¯cation task. For the ¯rst operation of the recognition task, the corpus is
preprocessed and some features are extracted by word embedding using the
word2vec model, then an LSTM neural detection model is built. In stage 1, the test
data are predicted using the recognition model. The work°ow of stage 2 is similar,
with the addition of using the predicted DDI pairs from stage 1 as training data for
type classi¯cation. Again, word embedding features are extracted, and classi¯cation
is performed. Detailed methods are described in the following sections.
4. Methods
The model architectures shown in Figs. 2 and 3 are representations of the machine
learning approaches used in this study, which are detailed in this section. It consists
of the following steps: (1) data preprocessing; (2) word embedding; and (3) model
building.
4.1. Data preprocessing
Since the annotated DDI corpus features segmentation of sentences and named entity tagging, the data preprocessing stage in the study is composed of deletion of
undesirable sentences, data abstraction, and stemming (Fig. 4).
1840027-7

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

Fig. 4. Flowchart of data preprocessing stage.

4.2. Deletion of undesirable sentences and data abstraction
If a sentence does not refer to drugs or to only one drug, no drug pairs can be found and
the deletion operation is applied to the sentence. Otherwise, the sentence is processed in
the data abstraction phase. Here, uppercase letters are changed to lowercase, and
numbers and punctuation marks are removed. The target drugs are replaced by the
symbol #, and other drugs are replaced by the symbol d. Take for example the sentence
\Barbiturates and glutethimide should not be administered to patients receiving coumarin drugs." It contains three drugs/drug types: barbiturates and coumarin. Because
the annotated drug pair in this sentence is barbiturates-coumarin, these are the
target drugs and are replaced by #. Glutethimide is replaced by d. The data abstraction result for this example sentence is shown in Table 3.
4.3. Stemming
Stemming is a procedure of transforming an in°ected form to its root form. For
example, \administered" will be mapped to the root form \administer" after
stemming. Stemming can group semantics pertaining to the same word and re°ect
additional information around the word variations. Porter's stemmer was applied in
this paper.4 Table 4 shows an example of stemming.
Table 3. Example of data abstraction.
Type
Original sentence
Data abstraction

Sentence
Barbiturates and glutethimide should not be administered to patients
receiving coumarin drugs.
# and d should not be administered to patients receiving #.
1840027-8

Extraction of drug–drug interaction using neural embedding
Table 4. Example of stemming.
Type
Data abstraction
Stemming

Sentence
# and d should not be administered to patients receiving #.
# and d should not be administered to patient receiving #.

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

4.4. Word embedding for deep learning
Neural embedding has been used with great success in Natural Language Processing
(NLP). It provides a compact representation that encapsulates word similarity and
can provide state-of-the-art performance in a range of linguistic tasks. The success of
neural embedding has prompted a signi¯cant amount of research into applications in
domains other than language.9 The experimental model architecture (Fig. 2) uses
word embedding for deep learning from neural networks. We trained our predicted
models with the word2vec toolkit.5 The toolkit implements both the skip-Gram and
CBOW approaches of Mikolov et al.,22,23 which constitute two e±cient alternatives to
the standard computation of the output word probability distributions, using a softmax classi¯er. Algorithm 1 represents feature vectorization using word embedding.
Given a number of output nodes W in the neural network, using a hierarchical
softmax has the advantage of evaluating the probability distribution of only log2 ðW Þ
rather than evaluating W output nodes. The hierarchical softmax uses a binary tree

1840027-9

W. J. Hou & B. Ceesay

representation of the output layer with the W words as its leaves. For each node, it
explicitly represents the relative probabilities of its child nodes. This structure
de¯nes a random walk that assigns probabilities to words. As an illustration, an
input sentence S can be represented as

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

S ¼ x1 þ x2 þ x3 þ    þ xl ;

ð1Þ

where the `þ' symbol is the concatenation operator and xi is a word in S at index i. In
general, S is represented as a concatenation of words. We considered multilayer
networks with N layers of hidden units that give a C-dimensional output vector.
Hierarchical softmax has been proposed to speed up training in the context of language models by Morin and Bengio,24 following prior work by Goodman.14 The concept
is to decompose the softmax layer into a binary tree with the words of the vocabulary at
its leaves, such that the probability of a word, given a context, can be decomposed into
probabilities of choosing the correct child at each node along the path from the root node
to that leaf. This reduces the number of necessary updates in vocabulary size from a
linear to a logarithmic term. Assuming a word xi in Eq. (1), xi can be reached by an
appropriate path from the root of the tree. Let nðxi ; jÞ be the jth node on the path from
the root to xi , and let Lðxi Þ be the length of this path, so the nðxi ; 1Þ = root and
nðxi ; Lðxy ÞÞ ¼ xi . In addition, for any inner node y, let chðyÞ be an arbitrary ¯xed child
of y and let ½t be 1 if t is true and 1 otherwise. Given these assumptions, the hierarchical
softmax, given output x and input word xI , de¯nes pðxjxI Þ as follows:

Y

LðxÞ1

pðxjxI Þ ¼

ð½nðx; j þ 1Þ ¼ chðnðx; jÞÞ:v 0|
nðx;jÞ vxI Þ;

ð2Þ

j¼1
1
where ðzÞ ¼ 1þexpðzÞ
and v 0nðx;jÞ and vxI are input and output vectors. The ith entry of
the output vector is computed as h ki ðxÞ which will be de¯ned later. It can be veri¯ed that
N
n¼1 pðxjxI Þ ¼ 1 for N output nodes. The value of pðxjxI Þ is used as the weight connecting node x and node xI .
For the output layer, we considered multilayer networks with N layers of hidden
units that yield an output layer with a C-dimensional vector. As shown in Eq. (3),
fi ðxÞ represents the output function for output node ni given an output layer with
input words x.

fi ðxÞ ¼

d
X

wij h N
j ðxÞ þ b;

i ¼ 1; . . . ; C;

ð3Þ

j¼1

where wij are the weights for the output layer, b the weight for bias, and typically the
node i at kth layer is de¯ned as
0
1
h ki ðxÞ ¼ Z @

m
X

wij xi þ bA;

j¼1

1840027-10

ð4Þ

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

Fig. 5. Model for embedding in deep architecture.

where Z is a nonlinear squashing function such as hyperbolic tangent tanh, m the
number of nodes in layer one (i.e. input layer) and wij is a weight notation between
node ni in layer k þ 1 and node nj in layer k. The value of wij is computed by Eq. (3)
of pðni jxI Þ. Here, we describe a standard, fully connected, multi-layer network.
However, prior knowledge about a particular problem could lead to the selection of
other network designs. Deep learning consists of learning a model with several layers
of nonlinear mapping as shown in Fig. 5.
In this paper, we consider multi-layer networks with layers of hidden units that
give a C-dimensional output vector using an LSTM algorithm. In the recognition
task, C was de¯ned as 2, and in the classi¯cation task for types, C was de¯ned as 5.
The pseudocode for implementing the deep learning approaches is illustrated in
Algorithm 2.
Semantic features and relations are very signi¯cant in automatic determination
and representation of the text. It is important to note that the most frequent DDI
type has a dominant impact on semantic properties and consequently on the overall
performance of our model. It is bene¯cial to cover the learning of such characteristics
during training phase. As an illustration, consider the following expression:
.

Barbiturates and glutethimide should not be administered to patients receiving coumarin drugs.

In this expression, the roles of each entity (barbiturates, glutethimide and
coumarin) and the relations between these entities can best be represented using
their semantic structure and relations, from which models can learn to classify them.
Our system is able to learn semantic features and relationships between lexical items
in the text. Algorithm 1 presents the processing of word embedding in our model
using word2vec. A simple method for investigating and learning the semantic
representations in text is to determine the closeness of contained words, a task for
which word2vec has proven very suited. As shown in Algorithm 2, our model uses the
semantic relatedness between words from word2vec embedding to automatically
learn patterns useful for classi¯cation.

1840027-11

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

Every lexical expression in the English language is made up of tokens not all
which are completely unrelated. Degree of relatedness among tokens play a signi¯cant role in determining an interaction for DDI extractions. Each word token should
be encoded into a vector that represents a point in a \word space". This is important
for a number of reasons, but the most intuitive reason is perhaps that there exists an
N-dimensional space that is su±cient to encode all semantics of our language. Each
dimension would encode a meaning that we transfer using speech. For instance,
semantic dimensions might indicate tense (past versus present versus future), count
(singular versus plural), and role (subjective versus objective). Below is the list of
notations used in this study to describe feature encoding processes.
m
Wi
wi
X
Y
Zi

Context size, set to 5.
ith column of X, the input vector representation of word wi .
Word i from the vocabulary V .
Input word matrix.
Output word matrix.
ith column of Y , the output vector representation of word wi .
1840027-12

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

If two di®erent words have similar contexts (i.e. similar words are likely to appear
around them), the model should output similar results for both. One way for the
network to output similar context predictions for these two words is through similar
word vectors. Our network is motivated to learn similar word vectors for these two
words in the event of similar contexts.
The simplest and most common representation of a word vector is the one-hot
vector, which represents every word as an IR jV j1 vector of 0s except for one 1 at the
index of that word in the sorted English language. In this notation, jV j is the size of
the vocabulary. Word vectors in this type of encoding would appear as the following,
where n ¼ jV j:
2
3
2
3
2
3
x11
x21
xn1
6 x12 7
6 x22 7
6 xn2 7
6
7
6
7
6
7
w1 ¼ 6 . 7; w2 ¼ 6 . 7    wn ¼ 6 . 7:
ð5Þ
4 .. 5
4 .. 5
4 .. 5
x1n
x2n
xnn
Each word can be represented as a complete independent entity. However, this word
representation engenders two important problems in our study.
(1) It does not directly provide any notion of word-to-word relationship or similarity.
(2) With a large vocabulary size, it can be of very high dimension and thus computationally costly.
To address these issues, we use the Skip–Gram model22 to reduce the size of this space
from R jV j to something smaller, and thereby ¯nd a subspace that encodes the relationships between words using context. To ¯nd word embeddings (otherwise known as
word vectors), we ¯rst loop over the dataset and accumulate word co-occurrence
counts in a matrix X. Our approach in this study consists of the following steps:
(1)
(2)
(3)
(4)
(5)

Generate one-hot input matrix X.
Create embedded word vectors for the context vc ¼ Wc from X.
Generate 2m score vectors, Zcm ; . . . ; Zc1 ; Zcþ1 ; . . . ; Zcþm using Z ¼ Yvc .
Turn each score into a probability, y ¼ softmaxðZÞ.
The generated probability vectors should match the true probabilities, i.e.
y ðcmÞ ; . . . ; y ðc1Þ ; y ðcþ1Þ ; . . . ; y ðcþmÞ , or the one-hot vectors of the actual output.

First, we set up the known parameters. Let the known parameters in our model be
the sentences represented by one-hot word vectors. The input one-hot vectors or
context for the center word wc are represented by x ðcÞ and the output by y ðcÞ . In the
Skip-Gram model, where there is only one output, this is called y, which is the onehot vector of the known center word. Given a word wi as input to this model, we
de¯ne a vector v such that the ith column of X is the n-dimensional embedded
vector. We denote the n  1 vector as vi .
Similarly, Y is de¯ned as the output word matrix. The jth row of Y is an
n-dimensional embedded vector for word wj when it is an output of the model. We
1840027-13

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

denote this row of Y as Zj . Note that we do, in fact, learn two vectors for every word
wi (i.e. input word vector Wi and output word vector Zi ). Given X and Y , the model
learns by creating an objective function. Feature patterns are encoded into embedding vectors from which the model can learn to classify. In trying to derive a probability from true values, it is useful to apply information theory for a measure of the
distance between two distributions. Here, we use cross entropy Hð^
y ; yÞ, a commonly
applied distance/loss measure. This choice is indicated in the discrete case by derivation from the formulation of the loss function:
Hð^
y ; yÞ ¼ 

jV j
X

yj logð^
y j Þ:

ð6Þ

j¼1

In the example case, y is a one-hot vector. Thus, we know that the above loss
simpli¯es to:
Hð^
y ; yÞ ¼ yi logð^
y i Þ:

ð7Þ

In this formulation, i is the index where the correct word's one-hot vector is 1.
Now consider the case where our prediction was perfect and thus y^i ¼ 1. We can
then calculate Hð^
y ; yÞ ¼ 1 logð1Þ ¼ 0. Thus, for a perfect prediction, we face no
penalty or loss. Now, consider the opposite case in which our prediction was very
bad and thus y^i ¼ 0:01. As before, we can calculate our loss as Hð^
y ; yÞ ¼
1 logð0:01Þ  4:605. We now need to generate an objective function to evaluate the
model. The loss function is changed to
E ¼  log P ðwcm ; . . . ; wc1 ; wcþ1 ; . . . ; wcþm jwc Þ
¼  log

2m
Y

ð8Þ

P ðwcmþj jwc Þ

ð9Þ

P ðZcmþj jvc Þ

ð10Þ

j¼0;j6¼m

¼  log

2m
Y
j¼0;j6¼m

¼  log

2m
Y
j¼0;j6¼m

¼

2m
X

T
expðZ cmþj
vc Þ

P jV j

k¼1

ð11Þ

expðZ kT vc Þ

T
Z cmþj
vc þ 2m log

j¼0;j6¼m

jV j
X

expðZ kT vc Þ;

ð12Þ

k¼1

where c is the index of the context word and m the context size. With this objective
function, we can compute the gradients with respect to the unknown parameters and
at each iteration update them via Stochastic Gradient Descent.26
In machine learning, dealing with imbalanced data gives rise to sampling-related
problems. The traditional way of dealing with imbalanced data in natural language

1840027-14

Extraction of drug–drug interaction using neural embedding

processing consists of one of the following:
.

Ignoring the problem.
Undersampling the majority types.
. Oversampling the minority types.
J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

.

However, each of these strategies has side e®ects on the classi¯cation result. Ignoring
the problem of imbalanced data leads to a biased classi¯er result. By undersampling,
we solve the type imbalance issue, and increase the sensitivity of classi¯er models.
However, the more imbalanced the dataset, the more samples will be discarded in
this case, thereby throwing away potentially useful information. This gives rise to
two issues:
.
.

Are we developing a poor classi¯er because we have insu±cient data?
Or are we simply relying on bad features with poor discriminative power, and
would therefore not gain any bene¯t from additional data?

Using oversampling, especially before cross-validation, can lead to apparently
near-perfect accuracy in the model classi¯cation result but may in fact engender
over¯titing. Several recent studies have looked into the problem of dealing with
imbalanced data in a neural network. Simpson et al.30 demonstrated that oversampled deep neural networks are more selective and learn faster and more robustly.
An LSTM based framework for handling multiclass imbalance in DGA (Domain
Generation Algorithm) botnet detection by Tran et al.32 presents a novel LSTM.MI
algorithm to combine both binary and multiclass classi¯cation models, where the
original LSTM is adapted to be cost-sensitive.

I1
H1
I2

O1

I3

Hn

Oc

In

Fig. 6. Adaptation of the learning model for type classi¯cation using word vectors of k dimensions.
1840027-15

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

In this study, we adopted the Minibatch SGD (Stochastic Gradient Descent) with
Strati¯ed oversampling35 for training our model for classi¯cation. While Oversampling is a common method to potentially improve model's trained on imbalanced
data, it is important to remember that incorrect oversampling can lead to false
con¯dence in the models generalization performance. In this study, oversampling was
applied only to the training data, and none of the information in the test data was
used to create synthetic observations. The main point of testing a model is to estimate how the model will generalize to new data. If the decision to put a model to the
test is based on how it performs on a training set, it is critical that oversampling is
performed correctly and not in the test phase.
Our neural model using stemmed words (see Sec. 4) is shown in Fig. 6, where Ii
represents an embedding word vector for token i in the set of tokens in stemmed
words.
5. Results and Discussion
The evaluation is relation-oriented and based on the standard precision, recall and
F-score metrics. A DDI is correctly classi¯ed only if the system is able to assign the
correct prediction and the correct type to it. The performance of systems to identify
which pairs of drugs interact (regardless of the type) is also evaluated. The three
metrics are calculated for each DDI types as follows:
Precision is the proportion of DDIs found by the learning system that are correct;
i.e. the ratio between the number of DDIs correctly detected (true positives) and the
total number of DDIs found by the system (true positives þ false positives).
P ¼

TP
:
TP þ FP

ð13Þ

Recall is the proportion of DDIs presented in the corpus that are found by the
system; i.e. the ratio between the number of DDIs correctly detected (true positives)
and the total number of drug entities in the gold standard (true positives+false
negatives).
R¼

TP
:
TP þ FN

ð14Þ

F-score is the harmonic mean of precision and recall.
F ¼

2RP
:
P þR

ð15Þ

Average measures are then computed. We consider the following metrics:
.

Micro-averaged measures are calculated by constructing a global contingency
table for DDI pairs then computing precision, recall and F-score.
. Macro-averaged measures are calculated from precision and recall computed for
each type and then averaged.
1840027-16

Extraction of drug–drug interaction using neural embedding

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Assume TPi represent true positive counts for type i, TNi represents true negative
counts for type i, FPi and FNi represents false positives counts and false negative
counts respectively for type i. For DDI pair detection with true positive TPd , false
positive FPd , false negative FNd and true negative TNd , the micro-averaged precision Pj and micro-averaged recall Rj are calculated as follows:
Pj ¼

TPd
;
TPd þ FPd

ð16Þ

Rj ¼

TPd
;
TPd þ FNd

ð17Þ

Fj ¼

2  P j  Rj
;
P j þ Rj

ð18Þ

where Fj is the micro-averaged F-score. In contrast, macro-averages for precision Pk
and recall Rk are calculated for all types as follow:
1
Pk ¼
n

Pn

1
n

Pn

Rk ¼

Pn

i¼1 TPi

;

ð19Þ

i¼1 TPi
;
ðTP
i þ FNi Þ
i¼1

ð20Þ

i¼1 ðTPi

Fk ¼

Pn

þ FPi Þ

2  P k  Rk
;
P k þ Rk

ð21Þ

where n is number of types and Fk is the macro-averaged F-score.
Thus, for example, the precision for mechanism relationships can be de¯ned as
the ratio between the number of DDIs correctly classi¯ed as mechanism and the total
number of DDIs so classi¯ed (including those wrongly assigned to this type). Similarly, recall for mechanism relationships is de¯ned as the ratio between the number of
DDIs correctly classi¯ed as mechanism and the total number of DDIs with this type
in the gold standard. The precision and recall for the rest of the DDIs types are
de¯ned in a similar manner.
Evaluating each DDI type separately allows us to assess the level of di±culty of
detecting and classifying each type of interaction. Additionally, it is important to
note that the scores achieved on the most frequent DDI type have a much greater
impact on overall performance than those achieved on DDI types with few instances.
Tables 5 and 6 represent the global contingency results for DDI pair detections for
our system, and the top ¯ve systems participating in Semeval 2013 with DrugBank
and Medline test datasets.1 Our system achieved the best results of 0.889 recall rate,
0.861 precision rate, and 0.875 F-score. In comparison, the ¯rst ranking system at
Semeval 2013 achieved 0.838 recall, 0.816 precision and 0.827 F-score (Table 5). For
the Medline dataset, our system also performed best (Table 6). Similarly, Tables 7
and 8 present the results for labeling of interactions for both DrugBank and Medline
corpora for macro-averages for all types. Our system ranked top 2nd and 1st,
1840027-17

W. J. Hou & B. Ceesay

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Table 5. DrugBank dataset micro-averaged
results for DDI pairs detection.
Team

Recall

Precision

F -score

Other Systems
FBK-irst
WBI
SCAI
UTurku
UC3M

0.838
0.755
0.681
0.638
0.758

0.816
0.814
0.796
0.843
0.656

0.827
0.783
0.734
0.726
0.703

Our System

0.889

0.861

0.875

Table 6. Medline dataset micro-averaged results
for DDI pairs detection.
Team

Recall

Precision

F -score

Other systems
FBK-irst
WBI
UWMTRIADS
SCAI
UC3M

0.505
0.421
0.630
0.526
0.642

0.558
0.625
0.387
0.431
0.313

0.530
0.503
0.479
0.474
0.421

Our system

0.505

0.558

0.530

Table 7. DrugBank dataset macro-averaged results.
Team

Recall

Precision

F -score

Other systems
FBK-irst
WBI
UTurku
NIL UCM
UC3M
UWMTRIADS

0.639
0.575
0.507
0.498
0.566
0.485

0.708
0.666
0.777
0.651
0.557
0.487

0.672
0.617
0.614
0.565
0.561
0.486

Our system

0.545

0.657

0.626

Table 8. Medline dataset macro-average results.
Team

Recall

Precision

F -score

Other systems
FBK-irst
WBI
UWMTRIADS
UCOLORADO SOM
SCAI

0.514
0.333
0.413
0.38
0.197

0.384
0.376
0.297
0.212
0.42

0.44
0.353
0.345
0.272
0.269

Our system

0.534

0.688

0.592

1840027-18

Extraction of drug–drug interaction using neural embedding

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Table 9. Our system performance on individual
types for the DrugBank dataset.
Types

Recall

Precision

F -score

Advice
E®ect
Int
Mechanism

0.540
0.403
0.780
0.456

0.610
0.524
0.845
0.647

0.702
0.455
0.811
0.535

Table 10. Our system performance on individual
types for the Medline dataset.
Types

Recall

Precision

F -score

Advice
E®ect
Int
Mechanism

0.463
0.681
0.414
0.576

0.517
0.783
0.821
0.629

0.489
0.728
0.550
0.601

respectively. Our system performance with respect to individual types with DrugBank and Medline datasets is shown in Tables 9 and 10. The overall evaluation
considered two aspects.
(1) System's performance in identifying interaction pairs.
(2) System's performance in classifying the interactions pairs into prede¯ned types.
It may be observed that there are signi¯cant di®erences in micro-averaged and
macro-averaged results for participating systems. Generally systems have better
results with the Drug-Bank dataset and weaker results with Medline. Participating
systems were also better at predicting interaction pairs than at identifying interaction types.
However, the results for the detection and classi¯cation for DDIs did not exceed
an F-score of 67% with recall approximately 20–70%. Evaluation results suggest that
some types of DDIs are more di±cult to classify than others. The F-score for advice
ranged from 49% to 81% (Tables 9 and 10). One possible explanation for this
could be that recommendations or advice regarding a DDI are typically described by
very similar text patterns, such as \DRUG should not be used in combination with
DRUG " or \Caution should be observed when DRUG is administered with DRUG ".
Another possible explanation comes from the coverage of the corpora. As shown in
Table 2, the number of true pairs for each type in the data is not large enough. For
example, there are only 8 pairs for type \advice". This minimal training data
presumably caused the weak performance in this category.
Since the corpus developed for DDIExtraction in Semeval 2013 Shared Task was
made up of texts from two di®erent sources (Medline and the DrugBank),27 the
di®erent approaches could be evaluated on two di®erent styles of biomedical texts.
While Medline abstracts are usually written in extremely scienti¯c language, texts
1840027-19

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

from DrugBank are written in a less technical form. Notably, the amount of training
data from Medline (1301 sentences including 1787 DDI pairs) was much smaller than
that from DrugBank (5675 sentences including 26,995 DDI pairs). The di®erence in
experimental results between these data sets indicates that the size of training data
sets is important. If we want the output to be bene¯cial to human curators, su±cient
training data is needed. With su±cient data, the model can learn su±cient features
and provide better performance. This causality likely explains the di®erences in
performance between the data sets.
The system in the present study adopts an automatic IE approach and no postprocessing is applied on the system's output, both general requirements of the
Semeval 2013 tasks. Overall, our system achieved a good performance compared to
other participating system (Tables 5–8).
6. Conclusions
DDI is an important phenomenon in the pharmaceutical industry. Extraction of DDI
information from biomedical text is a promising research approach towards understanding the e®ect of one drug in the presence of another. The large amount of
available data complicates understanding of DDIs and their e®ects. In this study, we
explored text mining methods for automatic extraction of DDI information from
text. By combining feature embedding with deep learning methods, we obtained
improved results in learning those features with a LSTM neural network. We conclude that information extraction can play a signi¯cant role in understanding
drug-drug interactions.
References
1. 2013 S, Extraction of Drug–Drug Interactions from BioMedical Texts, https://www.cs.
york.ac.uk/semeval-2013/task9/index.php%3Fid=evaluation.html/.
2. Abacha AB, Zweigenbaum P, Automatic extraction of semantic relations between
medical entities: A rule based approach, J Biomed Semant 2(5):S4, 2011.
3. Abdelaziz I, Fokoue A, Hassanzadeh O, Zhang P, Sadoghi M, Large-scale structural and
textual similarity-based mining of knowledge graph to predict drug–drug interactions,
Web Semant Sci Serv Agents World Wide Web 44:104–117, 2017.
4. Algorithm TPS, The Porter Stemming Algorithm, http://tartarus.org/martin/PorterStemmer/.
5. Archive GC, word2vec, https://code.google.com/archive/p/word2vec/.
6. Baxter K, Preston C, Stockley's Drug Interactions, Pharmaceutical Press, London, 2010.
7. Borgelt C, An implementation of the fp-growth algorithm, Proc 1st Int Workshop on
Open Source Data Mining: Frequent Pattern Mining Implementations, ACM, pp. 1–5,
2005.
8. Cao DS, Xiao N, Li YJ, Zeng WB, Liang YZ, Lu AP, Xu QS, Chen A, Integrating
multiple evidence sources to predict adverse drug reactions based on a systems pharmacology model, CPT Pharmacometrics Syst Pharmacol 4(9):498–506, 2015.
9. Chamberlain BP, Clough J, Deisenroth MP, Neural embeddings of graphs in hyperbolic
space, arXiv:170510359.
1840027-20

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

Extraction of drug–drug interaction using neural embedding

10. Dai HJ, Chang YC, Tsai RTH, Hsu WL, New challenges for biological text-mining in the
next decade, J Comput Sci Technol 25(1):169–179, 2010.
11. DailyMed, DailyMed Announcements, https://dailymed.nlm.nih.gov/dailymed/index.
cfm/.
12. Edwards IR, Aronson JK, Adverse drug reactions: De¯nitions, diagnosis, and management, The Lancet 356(9237):1255–1259, 2000.
13. Embarek M, Ferret O, Learning patterns for building resources about semantic relations
in the medical domain, Proceedings of the 6th Language Resources and Evaluation Conference (LREC 2008), pp. 2006–2012, 2008.
14. Goodman J, Classes for fast maximum entropy training, 2001 IEEE Int Conf Acoustics,
Speech, and Signal Processing (ICASSP'01), pp. 561–564, 2001.
15. Gopalakrishnan V, Lustgarten JL, Visweswaran S, Cooper GF, Bayesian rule learning for
biomedical data mining, Bioinformatics 26(5):668–675, 2010.
16. Hou WJ, Chen HY, Rule extraction in gene–disease relationship discovery, Gene
518(1):132–138, 2013.
17. Jung J, Lee D, Inferring disease association using clinical factors in a combinatorial
manner and their use in drug repositioning, Bioinformatics 29(16):2017–2023, 2013.
18. Kim JH, Mitchell A, Attwood TK, Hilario M, Learning to extract relations for protein
annotation, Bioinformatics 23(13):i256–i263, 2007.
19. Lee CH, Khoo CS, Na JC, Automatic identi¯cation of treatment relations for medical
ontology learning: An exploratory study, Proceedings of the 8th International ISKO
Conference, pp. 245–250, 2004.
20. McCray AT, The umls semantic network, Proc Symp Comput Appl Med Care, American
Medical Informatics Association, pp. 503–507, 1989.
21. Medscape, Drug Interaction Checker, https://reference.medscape.com/drug-interactionchecker/.
22. Mikolov T, Chen K, Corrado G, Dean J, E±cient estimation of word representations in
vector space, arXiv:13013781.
23. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J, Distributed representations of
words and phrases and their compositionality, Advances in Neural Information Processing Systems, pp. 3111–3119, 2013.
24. Morin F, Bengio Y, Hierarchical probabilistic neural network language model, Aistats,
Citeseer, pp. 246–252, 2005.
25. U.S. National Library of Medicine, Detailed Indexing Statistics: 1965–2017, https://
www.nlm.nih.gov/bsd/index stats comp.html/.
26. Šarić J, Jensen LJ, Ouzounova R, Rojas I, Bork P, Extraction of Regulatory Gene/
Protein Networks from Medline, 2005.
27. Segura-Bedmar I, Martínez P, de Pablo-Sanchez C, A linguistic rule-based approach to
extract drug-drug interactions from pharmacological documents, BMC Bioinformatics,
BioMed Central, p. S1, 2011.
28. Segura Bedmar I, Martinez P, Sanchez Cisneros D, The 1st ddi extraction-2011 challenge
task: Extraction of drug-drug interactions from biomedical texts, 2011.
29. Segura-Bedmar I, Martínez P, Zazo MH, Semeval-2013 task 9: Extraction of drug-drug
interactions from biomedical texts (ddi extraction 2013), Second Joint Conf Lexical and
Computational Semantics (*SEM), Vol 2: Proc Seventh Int Workshop on Semantic
Evaluation (SemEval 2013), pp. 341–350, 2013.
30. Simpson AJ, Over-sampling in a deep neural network, arXiv:150203648.
31. Sridhar D, Fakhraei S, Getoor L, A probabilistic approach for collective similarity-based
drug–drug interaction prediction, Bioinformatics 32(20):3175–3182, 2016.

1840027-21

J. Bioinform. Comput. Biol. 2018.16. Downloaded from www.worldscientific.com
by 209.6.235.112 on 09/30/21. Re-use and distribution is strictly not permitted, except for Open Access articles.

W. J. Hou & B. Ceesay

32. Tran D, Mac H, Tong V, Tran HA, Nguyen LG, A lstm based framework for handling
multiclass imbalance in dga botnet detection, Neurocomputing 275:2401–2413, 2018.
33. Vilar S, Uriarte E, Santana L, Lorberbaum T, Hripcsak G, Friedman C, Tatonetti NP,
Similarity-based modeling in large-scale prediction of drug–drug interactions, Nat Protoc
9(9):2147, 2014.
34. Wishart DS, Knox C, Guo AC, Shrivastava S, Hassanali M, Stothard P, Chang Z,
Woolsey J, Drugbank: A comprehensive resource for in silico drug discovery and exploration, Nucleic Acids Res 34(suppl 1):D668–D672, 2006.
35. Zhao P, Zhang T, Accelerating minibatch stochastic gradient descent using strati¯ed
sampling, arXiv:14053080, 2014.
36. Zheng W, Lin H, Zhao Z, Xu B, Zhang Y, Yang Z, Wang J, A graph kernel based on
context vectors for extracting drug–drug interactions, J Biomed Inform 61:34–43, 2016.

Wen-Juan Hou received a B.Sc. and an M.Sc. in Information
and Computer Education from the National Taiwan Normal
University, in Taipei, Taiwan, R.O.C., in 1991 and 1993, respectively. She completed a Ph.D. in Computer Science and Information Engineering from the National Taiwan University, in Taipei,
Taiwan, R.O.C., in 2007. She is currently the Assistant Professor
of the Department of Computer Science and Information Engineering at the National Taiwan Normal University. Her areas of
research interest include text mining in biomedical documents, semantic analysis in
texts, and related topics in natural language processing.
Bamfa Ceesay received a B.Sc in Computer Science and Information Engineering from the National Taipei University of
Technology, in Taipei, Taiwan R.O.C in 2011, followed by an
M.Sc. in Computer Science and Information Engineering from the
National Taiwan Normal University in 2014. His areas of research
interest include text mining in biomedical documents, event
extraction, bioinformatics, and natural language processing.

1840027-22

